{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7c29b145",
      "metadata": {
        "id": "7c29b145"
      },
      "source": [
        "#  AMTTP RAPIDS GPU Training Notebook (cuDF + XGBoost)\n",
        "\n",
        "Train the unified binary classifier fully on GPU, avoiding CPU one-hot and pandas copies. This notebook uses cuDF for GPU DataFrames and XGBoost's gpu_hist with DeviceQuantileDMatrix for memory efficiency."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PgyNM0IHpR_1"
      },
      "id": "PgyNM0IHpR_1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRdt-1SupSxy",
        "outputId": "36970f77-4b13-44d1-9ca8-d852790ae922"
      },
      "id": "XRdt-1SupSxy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee39fb5e",
      "metadata": {
        "id": "ee39fb5e"
      },
      "source": [
        "## 1) Install GPU libraries (Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "838a6245",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "838a6245",
        "outputId": "ab4a0e13-1fe6-4f17-8868-2961b666c97e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "xgboost 3.1.1\n",
            "sklearn 1.6.1\n",
            "cudf 25.06.00\n"
          ]
        }
      ],
      "source": [
        "# If running on Google Colab, install RAPIDS & deps\n",
        "# (Colab currently uses CUDA 12; these wheels target CUDA 12.x)\n",
        "!pip install -q cudf-cu12 rmm-cu12 cupy-cuda12x xgboost scikit-learn\n",
        "\n",
        "import sys, subprocess\n",
        "print(sys.version)\n",
        "!python -c \"import xgboost as xgb; import sklearn; import cudf, cupy, rmm; print('xgboost', xgb.__version__); print('sklearn', sklearn.__version__); print('cudf', cudf.__version__)\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b2020d9",
      "metadata": {
        "id": "3b2020d9"
      },
      "source": [
        "## 2) GPU check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3958ed4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3958ed4",
        "outputId": "f19d0b7b-cdb6-414c-b0e1-a06ae1e0b045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: NVIDIA A100-SXM4-80GB, Memory: 85.17 GB\n",
            "NVIDIA A100-SXM4-80GB, 81920 MiB\n"
          ]
        }
      ],
      "source": [
        "import cupy as cp\n",
        "dev = cp.cuda.Device(0)\n",
        "props = cp.cuda.runtime.getDeviceProperties(dev.id)\n",
        "name = props['name'].decode()\n",
        "mem_gb = props['totalGlobalMem']/1e9\n",
        "print(f'GPU: {name}, Memory: {mem_gb:.2f} GB')\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "946dbb67",
      "metadata": {
        "id": "946dbb67"
      },
      "source": [
        "## 3) Configure RMM memory pool (reduces fragmentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96e838df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96e838df",
        "outputId": "87ef8263-5357-42d1-b548-71414ec01d10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMM pool initialized: 59.31 GB\n"
          ]
        }
      ],
      "source": [
        "import rmm, cudf\n",
        "free_mem = cp.cuda.runtime.memGetInfo()[0]\n",
        "pool_size = int(free_mem * 0.7)\n",
        "# Ensure pool_size is a multiple of 256\n",
        "pool_size = (pool_size // 256) * 256\n",
        "rmm.reinitialize(pool_allocator=True, initial_pool_size=pool_size)\n",
        "print(f'RMM pool initialized: {pool_size/1e9:.2f} GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98f01da9",
      "metadata": {
        "id": "98f01da9"
      },
      "source": [
        "## 4) Parameters & paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c27d7c38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c27d7c38",
        "outputId": "50ab486d-2601-4706-8f91-2358916c7586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: /content/drive/MyDrive/promachine/merged_clean_unified.parquet\n",
            "Output directory: /content/drive/MyDrive/promachine\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# User-provided dataset path\n",
        "dataset_path = '/content/drive/MyDrive/promachine/merged_clean_unified.parquet'\n",
        "output_dir = Path('/content/drive/MyDrive/promachine')\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Optional downsampling for limited VRAM (None = use full dataset)\n",
        "sample_rows = None  # e.g., 1_500_000 on T4 if OOM\n",
        "\n",
        "# XGBoost training params (memory-conscious defaults)\n",
        "max_bin = 256\n",
        "rounds = 600\n",
        "early = 50\n",
        "max_depth = 6\n",
        "learning_rate = 0.1\n",
        "subsample = 0.8\n",
        "colsample_bytree = 0.7\n",
        "\n",
        "print('Dataset:', dataset_path)\n",
        "print('Output directory:', output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e38422",
      "metadata": {
        "id": "24e38422"
      },
      "source": [
        "## 5) Load dataset into GPU (cuDF) and optional sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f158bb08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f158bb08",
        "outputId": "57fb0eab-7701-4000-b33f-41482e3d66d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded cuDF: 2,926,538 rows x 67 cols\n",
            "Label counts: {0: 2882946, 1: 43592}\n"
          ]
        }
      ],
      "source": [
        "import cudf\n",
        "\n",
        "gdf = cudf.read_parquet(dataset_path)\n",
        "print(f'Loaded cuDF: {len(gdf):,} rows x {gdf.shape[1]} cols')\n",
        "\n",
        "if sample_rows is not None and len(gdf) > sample_rows:\n",
        "    gdf = gdf.sample(n=sample_rows, random_state=42).sort_index()\n",
        "    print(f'Sampled to: {len(gdf):,} rows')\n",
        "\n",
        "# Quick label check\n",
        "counts = gdf['label_unified'].value_counts()\n",
        "print('Label counts:', counts.to_pandas().to_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12911bd3",
      "metadata": {
        "id": "12911bd3"
      },
      "source": [
        "## 6) Prepare features on GPU (no one-hot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a7615f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a7615f8",
        "outputId": "37a27254-8c13-4854-9197-ff6c68fa02f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features prepared: num=56, cat=6, total=62\n"
          ]
        }
      ],
      "source": [
        "EXCLUDE_COLS = {'label', 'label_raw', 'FLAG', 'chain'}\n",
        "label_col = 'label_unified'\n",
        "\n",
        "keep_cols = [c for c in gdf.columns if c not in EXCLUDE_COLS and c != label_col]\n",
        "X = gdf[keep_cols]\n",
        "y = gdf[label_col].astype('int8')\n",
        "del gdf  # free memory\n",
        "\n",
        "# Identify categorical columns (strings/objects). cuDF often uses 'object' for strings.\n",
        "cat_cols = []\n",
        "for c in keep_cols:\n",
        "    dt = X[c].dtype\n",
        "    if str(dt) in ('object', 'str'):\n",
        "        cat_cols.append(c)\n",
        "\n",
        "# Convert to categorical dtype and handle missing values efficiently\n",
        "for c in cat_cols:\n",
        "    X[c] = X[c].astype('category')\n",
        "    X[c] = X[c].cat.add_categories(['__MISSING__']).fillna('__MISSING__')\n",
        "\n",
        "# Numeric columns: median imputation on GPU\n",
        "num_cols = [c for c in keep_cols if c not in cat_cols]\n",
        "for c in num_cols:\n",
        "    if X[c].isna().any():\n",
        "        X[c] = X[c].fillna(X[c].median())\n",
        "\n",
        "print(f'Features prepared: num={len(num_cols)}, cat={len(cat_cols)}, total={X.shape[1]}')\n",
        "# Build feature_types list for XGBoost\n",
        "feature_types = ['categorical' if c in cat_cols else 'float' for c in X.columns]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa0a07b9",
      "metadata": {
        "id": "fa0a07b9"
      },
      "source": [
        "## 7) Temporal split (70/15/15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30fbc799",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30fbc799",
        "outputId": "01c1f8d5-4386-4375-b350-662373ddb978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 2,048,576, Val: 438,981, Test: 438,981\n",
            "Train labels: {0: 2004984, 1: 43592}\n",
            "Val labels:   {0: 438981}\n",
            "Test labels:  {0: 438981}\n",
            "\n",
            "锔 Validation/test split is empty or single-class. Applying stratified fallback split...\n",
            "New Train: 2048576 labels {0: 2018062, 1: 30514}\n",
            "New Val:   438981 labels {0: 432442, 1: 6539}\n",
            "New Test:  438981 labels {0: 432442, 1: 6539}\n"
          ]
        }
      ],
      "source": [
        "n = len(X)\n",
        "train_end = int(n * 0.70)\n",
        "val_end = int(n * 0.85)\n",
        "\n",
        "# Initial temporal split (by row order)\n",
        "X_train, y_train = X.iloc[:train_end], y.iloc[:train_end]\n",
        "X_val, y_val = X.iloc[train_end:val_end], y.iloc[train_end:val_end]\n",
        "X_test, y_test = X.iloc[val_end:], y.iloc[val_end:]\n",
        "\n",
        "print(f'Train: {len(X_train):,}, Val: {len(X_val):,}, Test: {len(X_test):,}')\n",
        "\n",
        "# Label distribution per split\n",
        "train_counts = y_train.value_counts()\n",
        "val_counts = y_val.value_counts()\n",
        "test_counts = y_test.value_counts()\n",
        "print('Train labels:', {int(k): int(v) for k, v in train_counts.to_pandas().items()})\n",
        "print('Val labels:  ', {int(k): int(v) for k, v in val_counts.to_pandas().items()})\n",
        "print('Test labels: ', {int(k): int(v) for k, v in test_counts.to_pandas().items()})\n",
        "\n",
        "# Fallback: if val or test is empty or single-class, redo a stratified split\n",
        "need_strat = (\n",
        "    len(X_val) == 0 or len(X_test) == 0 or\n",
        "    int(y_val.nunique()) < 2 or int(y_test.nunique()) < 2\n",
        ")\n",
        "\n",
        "if need_strat:\n",
        "    print('\\n锔 Validation/test split is empty or single-class. Applying stratified fallback split...')\n",
        "    import numpy as np\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    idx = np.arange(n)\n",
        "    y_np = y.to_pandas().values\n",
        "\n",
        "    # Train vs temp (70/30) with stratification\n",
        "    idx_train, idx_temp, y_train_np, y_temp_np = train_test_split(\n",
        "        idx, y_np, test_size=0.30, random_state=42, stratify=y_np\n",
        ")\n",
        "    # Temp into val/test (50/50 of 30%) with stratification -> 15%/15%\n",
        "    idx_val, idx_test, y_val_np, y_test_np = train_test_split(\n",
        "        idx_temp, y_temp_np, test_size=0.50, random_state=42, stratify=y_temp_np\n",
        ")\n",
        "\n",
        "    # Apply indices to cuDF\n",
        "    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
        "    X_val, y_val = X.iloc[idx_val], y.iloc[idx_val]\n",
        "    X_test, y_test = X.iloc[idx_test], y.iloc[idx_test]\n",
        "\n",
        "    # Recompute and report label counts\n",
        "    train_counts = y_train.value_counts(); val_counts = y_val.value_counts(); test_counts = y_test.value_counts()\n",
        "    print('New Train:', len(X_train), 'labels', {int(k): int(v) for k, v in train_counts.to_pandas().items()})\n",
        "    print('New Val:  ', len(X_val), 'labels', {int(k): int(v) for k, v in val_counts.to_pandas().items()})\n",
        "    print('New Test: ', len(X_test), 'labels', {int(k): int(v) for k, v in test_counts.to_pandas().items()})\n",
        "\n",
        "# Note: We no longer delete X and y here to allow safe re-runs of this cell."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2b3fa6e",
      "metadata": {
        "id": "f2b3fa6e"
      },
      "source": [
        "## 8) Train XGBoost on GPU (DeviceQuantileDMatrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e7205a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65e7205a",
        "outputId": "fa61abfc-082c-4740-9c56-c4ad46f72d73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scale_pos_weight: 66.14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [00:28:23] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"enable_categorical\", \"predictor\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-aucpr:0.05243\tval-aucpr:0.05429\n",
            "[50]\ttrain-aucpr:0.35534\tval-aucpr:0.34519\n",
            "[100]\ttrain-aucpr:0.50738\tval-aucpr:0.48148\n",
            "[150]\ttrain-aucpr:0.61536\tval-aucpr:0.57038\n",
            "[200]\ttrain-aucpr:0.71704\tval-aucpr:0.65531\n",
            "[250]\ttrain-aucpr:0.80384\tval-aucpr:0.72301\n",
            "[300]\ttrain-aucpr:0.84660\tval-aucpr:0.75974\n",
            "[350]\ttrain-aucpr:0.87948\tval-aucpr:0.79076\n",
            "[400]\ttrain-aucpr:0.90681\tval-aucpr:0.81584\n",
            "[450]\ttrain-aucpr:0.91822\tval-aucpr:0.83011\n",
            "[500]\ttrain-aucpr:0.92424\tval-aucpr:0.83948\n",
            "[550]\ttrain-aucpr:0.93435\tval-aucpr:0.85033\n",
            "[599]\ttrain-aucpr:0.93698\tval-aucpr:0.85621\n",
            "Best iteration: 599\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Compute class weight ratio on train split\n",
        "pos = int((y_train == 1).sum())\n",
        "neg = int((y_train == 0).sum())\n",
        "scale_pos_weight = (neg / max(pos, 1)) if pos > 0 else 1.0\n",
        "print('scale_pos_weight:', round(scale_pos_weight, 2))\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
        "dval = xgb.DMatrix(X_val, label=y_val, enable_categorical=True)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'tree_method': 'hist',  # Changed from 'gpu_hist' to 'hist'\n",
        "    'predictor': 'gpu_predictor',\n",
        "    'max_depth': max_depth,\n",
        "    'learning_rate': learning_rate,\n",
        "    'subsample': subsample,\n",
        "    'colsample_bytree': colsample_bytree,\n",
        "    # 'sampling_method': 'gradient_based', # Removed this line\n",
        "    'max_bin': max_bin,\n",
        "    'scale_pos_weight': scale_pos_weight,\n",
        "    'eval_metric': 'aucpr',\n",
        "    'enable_categorical': True,\n",
        "}\n",
        "\n",
        "evals = [(dtrain, 'train'), (dval, 'val')]\n",
        "bst = xgb.train(params, dtrain, num_boost_round=rounds, evals=evals,\n",
        "                early_stopping_rounds=early, verbose_eval=50)\n",
        "print('Best iteration:', bst.best_iteration)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbf06559",
      "metadata": {
        "id": "dbf06559"
      },
      "source": [
        "## 9) Evaluate on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66ab08f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66ab08f9",
        "outputId": "5e476d1f-e717-448b-c404-72bbaa9b371d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "auc_roc: 0.9917\n",
            "auc_pr: 0.8483\n",
            "f1: 0.6144\n",
            "balanced_acc: 0.9453\n",
            "accuracy: 0.9831\n",
            "sensitivity: 0.9064\n",
            "specificity: 0.9842\n",
            "precision: 0.4647\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'auc_roc': 0.9917146712219832,\n",
              " 'auc_pr': 0.848307256877262,\n",
              " 'f1': 0.61441973772871,\n",
              " 'balanced_acc': 0.9453103096949387,\n",
              " 'accuracy': 0.9830539362751463,\n",
              " 'sensitivity': 0.9064077076005506,\n",
              " 'specificity': 0.9842129117893267,\n",
              " 'precision': 0.46471695154461345}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, balanced_accuracy_score, confusion_matrix, accuracy_score\n",
        "\n",
        "y_pred = bst.predict(dtest, iteration_range=(0, bst.best_iteration + 1))\n",
        "y_true = y_test.to_numpy()\n",
        "y_prob = np.asarray(y_pred)\n",
        "y_hat = (y_prob >= 0.5).astype(np.int32)\n",
        "\n",
        "metrics = {\n",
        "    'auc_roc': float(roc_auc_score(y_true, y_prob)),\n",
        "    'auc_pr': float(average_precision_score(y_true, y_prob)),\n",
        "    'f1': float(f1_score(y_true, y_hat)),\n",
        "    'balanced_acc': float(balanced_accuracy_score(y_true, y_hat)),\n",
        "    'accuracy': float(accuracy_score(y_true, y_hat)),\n",
        "}\n",
        "tn, fp, fn, tp = confusion_matrix(y_true, y_hat).ravel()\n",
        "metrics['sensitivity'] = float(tp / (tp + fn)) if (tp + fn) else 0.0\n",
        "metrics['specificity'] = float(tn / (tn + fp)) if (tn + fp) else 0.0\n",
        "metrics['precision'] = float(tp / (tp + fp)) if (tp + fp) else 0.0\n",
        "\n",
        "for k, v in metrics.items():\n",
        "    print(f'{k}: {v:.4f}')\n",
        "\n",
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rOr1dKNG96ZY",
      "metadata": {
        "id": "rOr1dKNG96ZY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a3dd0bd4",
      "metadata": {
        "id": "a3dd0bd4"
      },
      "source": [
        "## Install PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1bc269c",
      "metadata": {
        "id": "c1bc269c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de39aa7-5502-4d31-ac2c-ef0f54b9ce39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8f13e83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8f13e83",
        "outputId": "d0420009-80c1-47c1-a8db-6593016dd5e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.49.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.3)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.1)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from shap) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.43.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->shap) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.5.0\n"
          ]
        }
      ],
      "source": [
        "# Install additional libraries for hyperparameter tuning and explainability\n",
        "!pip install optuna shap plotly matplotlib seaborn wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna-integration[wandb]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VDnONCPcTGC",
        "outputId": "13a85e2b-bb1f-4329-99a7-fa67444cdb4a"
      },
      "id": "0VDnONCPcTGC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna-integration[wandb]\n",
            "  Downloading optuna_integration-4.5.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (from optuna-integration[wandb]) (4.5.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from optuna-integration[wandb]) (0.22.3)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna->optuna-integration[wandb]) (1.17.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna->optuna-integration[wandb]) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna->optuna-integration[wandb]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna->optuna-integration[wandb]) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna->optuna-integration[wandb]) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna->optuna-integration[wandb]) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna->optuna-integration[wandb]) (6.0.3)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb->optuna-integration[wandb]) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->optuna-integration[wandb]) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->optuna-integration[wandb]) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb->optuna-integration[wandb]) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb->optuna-integration[wandb]) (2.11.10)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->optuna-integration[wandb]) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->optuna-integration[wandb]) (2.43.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb->optuna-integration[wandb]) (4.15.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna->optuna-integration[wandb]) (1.3.10)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->optuna-integration[wandb]) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->optuna-integration[wandb]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->optuna-integration[wandb]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->optuna-integration[wandb]) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb->optuna-integration[wandb]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb->optuna-integration[wandb]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb->optuna-integration[wandb]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb->optuna-integration[wandb]) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna->optuna-integration[wandb]) (3.2.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->optuna-integration[wandb]) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna->optuna-integration[wandb]) (3.0.3)\n",
            "Downloading optuna_integration-4.5.0-py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: optuna-integration\n",
            "Successfully installed optuna-integration-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aaf7651",
      "metadata": {
        "id": "7aaf7651"
      },
      "source": [
        "## Experiment tracking (Weights & Biases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0a8b38f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0a8b38f",
        "outputId": "9510a6ea-a36d-4275-b1f6-723c0a27aa11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 路路路路路路路路路路\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msegettii\u001b[0m (\u001b[33msegettii-segetii\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B configured for project='amttp-rapids-gpu' (mode=online)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from typing import Optional, List\n",
        "import wandb\n",
        "from optuna.integration.wandb import WeightsAndBiasesCallback\n",
        "\n",
        "# Configure Weights & Biases defaults\n",
        "WANDB_PROJECT = os.environ.get(\"WANDB_PROJECT\", \"amttp-rapids-gpu\")\n",
        "WANDB_ENTITY = os.environ.get(\"WANDB_ENTITY\")\n",
        "WANDB_GROUP = os.environ.get(\"WANDB_GROUP\", \"gpu_pipeline\")\n",
        "WANDB_MODE = os.environ.get(\"WANDB_MODE\", \"online\").lower()\n",
        "\n",
        "if WANDB_MODE in {\"offline\", \"disabled\"}:\n",
        "    os.environ[\"WANDB_MODE\"] = WANDB_MODE  # respected by wandb.init\n",
        "\n",
        "base_wandb_kwargs = {\n",
        "    'project': WANDB_PROJECT,\n",
        "}\n",
        "if WANDB_ENTITY:\n",
        "    base_wandb_kwargs['entity'] = WANDB_ENTITY\n",
        "\n",
        "try:\n",
        "    if WANDB_MODE not in {\"disabled\"}:\n",
        "        wandb.login()\n",
        "except Exception as exc:  # pragma: no cover\n",
        "    print(f\"W&B login skipped/failed: {exc}\")\n",
        "\n",
        "\n",
        "def start_wandb_run(run_name: str, config: Optional[dict] = None, group: Optional[str] = None,\n",
        "                    job_type: Optional[str] = None, tags: Optional[List[str]] = None):\n",
        "    \"\"\"Utility to (re)start a wandb run with shared defaults.\"\"\"\n",
        "    if os.environ.get(\"WANDB_MODE\", \"online\").lower() == \"disabled\":\n",
        "        return None\n",
        "\n",
        "    if wandb.run is not None:\n",
        "        wandb.finish()\n",
        "\n",
        "    wandb_kwargs = dict(base_wandb_kwargs)\n",
        "    wandb_kwargs.update({\n",
        "        'name': run_name,\n",
        "        'config': config or {},\n",
        "        'group': group or WANDB_GROUP,\n",
        "        'job_type': job_type,\n",
        "        'reinit': True,\n",
        "        'tags': tags,\n",
        "    })\n",
        "    return wandb.init(**{k: v for k, v in wandb_kwargs.items() if v is not None})\n",
        "\n",
        "\n",
        "wandb_optuna_kwargs = dict(base_wandb_kwargs)\n",
        "wandb_optuna_kwargs.setdefault('group', f\"optuna-{WANDB_GROUP}\")\n",
        "\n",
        "print(f\"W&B configured for project='{WANDB_PROJECT}' (mode={os.environ.get('WANDB_MODE', 'online')})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PVC5ep-pdkkD"
      },
      "id": "PVC5ep-pdkkD"
    },
    {
      "cell_type": "markdown",
      "id": "121dbebd",
      "metadata": {
        "id": "121dbebd"
      },
      "source": [
        "## Data Preparation for PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4990ea25",
      "metadata": {
        "id": "4990ea25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c267944-a33b-4899-bd0f-01801c948564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Tensor shapes:\n",
            "X_train: torch.Size([2048576, 62]) y_train: torch.Size([2048576, 1])\n",
            "X_val: torch.Size([438981, 62]) y_val: torch.Size([438981, 1])\n",
            "X_test: torch.Size([438981, 62]) y_test: torch.Size([438981, 1])\n",
            "\n",
            "DataLoaders created with batch size: 1024\n",
            "Number of categorical features: 6\n",
            "Number of numerical features: 56\n",
            "Total features: 62\n",
            "\n",
            "Feature encoding complete. Total features: 62\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd # Import pandas\n",
        "\n",
        "# Convert categorical features to numeric codes for PyTorch compatibility\n",
        "# Convert cuDF DataFrames to pandas DataFrames first\n",
        "X_train_encoded = X_train.to_pandas().copy()\n",
        "X_val_encoded = X_val.to_pandas().copy()\n",
        "X_test_encoded = X_test.to_pandas().copy()\n",
        "\n",
        "# Store mapping for later use\n",
        "cat_mappings = {}\n",
        "for col in cat_cols:\n",
        "    # Get unique categories and create mapping\n",
        "    # Convert cuDF Index to pandas Index before iterating\n",
        "    unique_cats = X_train[col].cat.categories.to_pandas()\n",
        "    cat_mappings[col] = {cat: idx for idx, cat in enumerate(unique_cats)}\n",
        "\n",
        "    # Apply encoding to all splits\n",
        "    X_train_encoded[col] = X_train[col].cat.codes.astype('float32').to_numpy() # Convert cuDF Series to numpy array\n",
        "    X_val_encoded[col] = X_val[col].cat.codes.astype('float32').to_numpy()     # Convert cuDF Series to numpy array\n",
        "    X_test_encoded[col] = X_test[col].cat.codes.astype('float32').to_numpy()   # Convert cuDF Series to numpy array\n",
        "\n",
        "\n",
        "# Convert pandas DataFrames to numpy arrays\n",
        "X_train_np = X_train_encoded.values.astype('float32')\n",
        "y_train_np = y_train.to_numpy().astype('float32') # y is already a cuDF Series\n",
        "\n",
        "X_val_np = X_val_encoded.values.astype('float32')\n",
        "y_val_np = y_val.to_numpy().astype('float32') # y is already a cuDF Series\n",
        "\n",
        "X_test_np = X_test_encoded.values.astype('float32')\n",
        "y_test_np = y_test.to_numpy().astype('float32') # y is already a cuDF Series\n",
        "\n",
        "\n",
        "# Convert to PyTorch Tensors\n",
        "X_train_pt = torch.from_numpy(X_train_np)\n",
        "y_train_pt = torch.from_numpy(y_train_np).unsqueeze(1)\n",
        "\n",
        "X_val_pt = torch.from_numpy(X_val_np)\n",
        "y_val_pt = torch.from_numpy(y_val_np).unsqueeze(1)\n",
        "\n",
        "X_test_pt = torch.from_numpy(X_test_np)\n",
        "y_test_pt = torch.from_numpy(y_test_np).unsqueeze(1)\n",
        "\n",
        "print(\"PyTorch Tensor shapes:\")\n",
        "print(\"X_train:\", X_train_pt.shape, \"y_train:\", y_train_pt.shape)\n",
        "print(\"X_val:\", X_val_pt.shape, \"y_val:\", y_val_pt.shape)\n",
        "print(\"X_test:\", X_test_pt.shape, \"y_test:\", y_test_pt.shape)\n",
        "\n",
        "# Create PyTorch Datasets and DataLoaders\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "batch_size = 1024  # Adjust batch size based on GPU memory\n",
        "\n",
        "train_dataset = TensorDataset(X_train_pt, y_train_pt)\n",
        "val_dataset = TensorDataset(X_val_pt, y_val_pt)\n",
        "test_dataset = TensorDataset(X_test_pt, y_test_pt)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"\\nDataLoaders created with batch size: {batch_size}\")\n",
        "print(f\"Number of categorical features: {len(cat_cols)}\")\n",
        "print(f\"Number of numerical features: {len(num_cols)}\")\n",
        "print(f\"Total features: {X_train_pt.shape[1]}\")\n",
        "\n",
        "# Store feature information for later use\n",
        "n_features = X_train_pt.shape[1]\n",
        "print(f\"\\nFeature encoding complete. Total features: {n_features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01aa88c1",
      "metadata": {
        "id": "01aa88c1"
      },
      "source": [
        "## Autoencoder for Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ab00b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3ab00b6",
        "outputId": "70cc5070-6564-439e-d4bb-f66d4ab13963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Autoencoder architecture:\n",
            "Input dimension: 62\n",
            "Encoding dimension: 16\n",
            "Compression ratio: 3.88x\n",
            "Autoencoder(\n",
            "  (encoder): Sequential(\n",
            "    (0): Linear(in_features=62, out_features=31, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=31, out_features=15, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.2, inplace=False)\n",
            "    (6): Linear(in_features=15, out_features=16, bias=True)\n",
            "    (7): ReLU()\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Linear(in_features=16, out_features=15, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=15, out_features=31, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.2, inplace=False)\n",
            "    (6): Linear(in_features=31, out_features=62, bias=True)\n",
            "    (7): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, encoding_dim, dropout_rate=0.2):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(input_dim // 2, input_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(input_dim // 4, encoding_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, input_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(input_dim // 4, input_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(input_dim // 2, input_dim),\n",
        "            nn.Sigmoid()  # Assuming normalized input\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded, encoded\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Normalize features for autoencoder training\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_np)\n",
        "X_val_scaled = scaler.transform(X_val_np)\n",
        "X_test_scaled = scaler.transform(X_test_np)\n",
        "\n",
        "# Convert to tensors and move to device\n",
        "X_train_scaled_pt = torch.from_numpy(X_train_scaled.astype('float32')).to(device)\n",
        "X_val_scaled_pt = torch.from_numpy(X_val_scaled.astype('float32')).to(device)\n",
        "X_test_scaled_pt = torch.from_numpy(X_test_scaled.astype('float32')).to(device)\n",
        "\n",
        "# Initialize autoencoder\n",
        "encoding_dim = max(16, n_features // 8)  # Adaptive encoding dimension\n",
        "autoencoder = Autoencoder(n_features, encoding_dim).to(device)\n",
        "\n",
        "print(f\"Autoencoder architecture:\")\n",
        "print(f\"Input dimension: {n_features}\")\n",
        "print(f\"Encoding dimension: {encoding_dim}\")\n",
        "print(f\"Compression ratio: {n_features/encoding_dim:.2f}x\")\n",
        "print(autoencoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ce7dbb5"
      },
      "source": [
        "def train_autoencoder(model, train_data, val_data, epochs=100, lr=0.001, patience=10, wandb_run=None):\n",
        "    \"\"\"Trains the autoencoder model.\"\"\"\n",
        "    criterion = nn.MSELoss() # Using MSE for reconstruction loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # Create DataLoaders for the scaled data\n",
        "    train_dataset_scaled = TensorDataset(train_data)\n",
        "    val_dataset_scaled = TensorDataset(val_data)\n",
        "\n",
        "    # Use a smaller batch size for autoencoder training if needed\n",
        "    autoencoder_batch_size = 512\n",
        "    train_loader_scaled = DataLoader(train_dataset_scaled, batch_size=autoencoder_batch_size, shuffle=True)\n",
        "    val_loader_scaled = DataLoader(val_dataset_scaled, batch_size=autoencoder_batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        for batch_x in train_loader_scaled:\n",
        "            batch_x = batch_x[0] # DataLoader returns a tuple\n",
        "            optimizer.zero_grad()\n",
        "            decoded, _ = model(batch_x)\n",
        "            loss = criterion(decoded, batch_x)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_train_loss += loss.item()\n",
        "\n",
        "        epoch_train_loss = running_train_loss / max(len(train_loader_scaled), 1)\n",
        "        train_losses.append(epoch_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_x in val_loader_scaled:\n",
        "                batch_x = batch_x[0] # DataLoader returns a tuple\n",
        "                decoded, _ = model(batch_x)\n",
        "                loss = criterion(decoded, batch_x)\n",
        "                running_val_loss += loss.item()\n",
        "\n",
        "        epoch_val_loss = running_val_loss / max(len(val_loader_scaled), 1)\n",
        "        val_losses.append(epoch_val_loss)\n",
        "\n",
        "        scheduler.step(epoch_val_loss)\n",
        "\n",
        "        if wandb_run is not None:\n",
        "            wandb.log({'autoencoder/train_loss': epoch_train_loss,\n",
        "                       'autoencoder/val_loss': epoch_val_loss,\n",
        "                       'autoencoder/epoch': epoch})\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "             print(f'Epoch {epoch}, Train Loss: {epoch_train_loss:.6f}, Val Loss: {epoch_val_loss:.6f}')\n",
        "\n",
        "        # Early stopping\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), output_dir / 'best_autoencoder.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch}')\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(output_dir / 'best_autoencoder.pth'))\n",
        "    return train_losses, val_losses"
      ],
      "id": "4ce7dbb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training autoencoder...\")\n",
        "autoencoder_run = start_wandb_run(\n",
        "    run_name=\"autoencoder-training\",\n",
        "    config={'epochs': 100, 'lr': 0.001, 'patience': 10, 'encoding_dim': encoding_dim},\n",
        "    job_type=\"autoencoder\",\n",
        "    tags=['autoencoder', 'feature-learning']\n",
        ")\n",
        "train_losses, val_losses = train_autoencoder(\n",
        "    autoencoder, X_train_scaled_pt, X_val_scaled_pt,\n",
        "    epochs=100, lr=0.001, patience=10,\n",
        "    wandb_run=autoencoder_run\n",
        ")\n",
        "\n",
        "if autoencoder_run is not None:\n",
        "    wandb.log({'autoencoder/final_train_loss': train_losses[-1],\n",
        "               'autoencoder/final_val_loss': val_losses[-1]})\n",
        "    wandb.finish()\n",
        "\n",
        "# Generate encoded features\n",
        "autoencoder.eval()\n",
        "with torch.no_grad():\n",
        "    encoded_train = autoencoder.encode(X_train_scaled_pt).cpu().numpy()\n",
        "    encoded_val = autoencoder.encode(X_val_scaled_pt).cpu().numpy()\n",
        "    encoded_test = autoencoder.encode(X_test_scaled_pt).cpu().numpy()\n",
        "\n",
        "print(f\"\\nAutoencoder training completed!\")\n",
        "print(f\"Original features: {n_features}\")\n",
        "print(f\"Encoded features: {encoded_train.shape[1]}\")\n",
        "print(f\"Final reconstruction loss - Train: {train_losses[-1]:.6f}, Val: {val_losses[-1]:.6f}\")\n",
        "\n",
        "# Combine original and encoded features\n",
        "X_train_enhanced = np.concatenate([X_train_np, encoded_train], axis=1)\n",
        "X_val_enhanced = np.concatenate([X_val_np, encoded_val], axis=1)\n",
        "X_test_enhanced = np.concatenate([X_test_np, encoded_test], axis=1)\n",
        "\n",
        "print(f\"Enhanced feature dimensions: {X_train_enhanced.shape[1]} (original: {n_features} + encoded: {encoded_train.shape[1]})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "id": "Laa3PiGwfFXp",
        "outputId": "8f7ef19f-05aa-46b6-9d48-c3293cef5e17"
      },
      "id": "Laa3PiGwfFXp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training autoencoder...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251106_194511-tk2ppivc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/tk2ppivc' target=\"_blank\">autoencoder-training</a></strong> to <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/tk2ppivc' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/tk2ppivc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Train Loss: 0.808357, Val Loss: 1.054081\n",
            "Epoch 10, Train Loss: 0.788455, Val Loss: 1.044888\n",
            "Epoch 20, Train Loss: 0.787518, Val Loss: 1.044537\n",
            "Epoch 30, Train Loss: 0.787113, Val Loss: 1.044265\n",
            "Epoch 40, Train Loss: 0.786884, Val Loss: 1.044078\n",
            "Epoch 50, Train Loss: 0.786835, Val Loss: 1.044061\n",
            "Epoch 60, Train Loss: 0.786747, Val Loss: 1.044053\n",
            "Epoch 70, Train Loss: 0.786901, Val Loss: 1.044050\n",
            "Epoch 80, Train Loss: 0.786867, Val Loss: 1.044051\n",
            "Early stopping at epoch 83\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>autoencoder/epoch</td><td></td></tr><tr><td>autoencoder/final_train_loss</td><td></td></tr><tr><td>autoencoder/final_val_loss</td><td></td></tr><tr><td>autoencoder/train_loss</td><td></td></tr><tr><td>autoencoder/val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>autoencoder/epoch</td><td>83</td></tr><tr><td>autoencoder/final_train_loss</td><td>0.78679</td></tr><tr><td>autoencoder/final_val_loss</td><td>1.04405</td></tr><tr><td>autoencoder/train_loss</td><td>0.78679</td></tr><tr><td>autoencoder/val_loss</td><td>1.04405</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">autoencoder-training</strong> at: <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/tk2ppivc' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/tk2ppivc</a><br> View project at: <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251106_194511-tk2ppivc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Autoencoder training completed!\n",
            "Original features: 62\n",
            "Encoded features: 16\n",
            "Final reconstruction loss - Train: 0.786790, Val: 1.044050\n",
            "Enhanced feature dimensions: 78 (original: 62 + encoded: 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e726554",
      "metadata": {
        "id": "8e726554"
      },
      "source": [
        "## Hyperparameter Tuning with Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a1e3c11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "9a1e3c11",
        "outputId": "50406b41-3e14-4c8b-a859-687d11b989ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting XGBoost hyperparameter optimization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4245658056.py:58: ExperimentalWarning: WeightsAndBiasesCallback is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  wandb_xgb_callback = WeightsAndBiasesCallback(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251106_201633-1boex9th</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/1boex9th' target=\"_blank\">sandy-smoke-13</a></strong> to <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/1boex9th' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/1boex9th</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-06 20:16:34,865] A new study created in memory with name: xgb_enhanced\n",
            "[I 2025-11-06 20:18:50,522] Trial 0 finished with value: 0.28996366070857693 and parameters: {'max_depth': 3, 'learning_rate': 0.029798264137986034, 'subsample': 0.6291310617712337, 'colsample_bytree': 0.7557265132559413, 'min_child_weight': 3, 'gamma': 0.32180330849243266, 'reg_alpha': 0.8319797826022152, 'reg_lambda': 0.7154609537560225, 'max_bin': 233, 'n_estimators': 701}. Best is trial 0 with value: 0.28996366070857693.\n",
            "[I 2025-11-06 20:20:14,115] Trial 1 finished with value: 0.2968668390154176 and parameters: {'max_depth': 6, 'learning_rate': 0.014195847390735651, 'subsample': 0.9841911500888477, 'colsample_bytree': 0.9428278923837955, 'min_child_weight': 5, 'gamma': 0.5437473353282765, 'reg_alpha': 0.4617469828507418, 'reg_lambda': 0.7344155621143443, 'max_bin': 355, 'n_estimators': 352}. Best is trial 1 with value: 0.2968668390154176.\n",
            "[I 2025-11-06 20:21:19,970] Trial 2 finished with value: 0.47841486010233536 and parameters: {'max_depth': 6, 'learning_rate': 0.15154248381968963, 'subsample': 0.6268879038634475, 'colsample_bytree': 0.7976301384362897, 'min_child_weight': 8, 'gamma': 0.07337726194844674, 'reg_alpha': 0.5973048175712456, 'reg_lambda': 0.6315337704485287, 'max_bin': 168, 'n_estimators': 273}. Best is trial 2 with value: 0.47841486010233536.\n",
            "[I 2025-11-06 20:22:39,473] Trial 3 finished with value: 0.468330384039364 and parameters: {'max_depth': 10, 'learning_rate': 0.017788402147130726, 'subsample': 0.9724544938464206, 'colsample_bytree': 0.6036589231262224, 'min_child_weight': 9, 'gamma': 0.6371835255620487, 'reg_alpha': 0.45594888214520135, 'reg_lambda': 0.29976507986114376, 'max_bin': 360, 'n_estimators': 236}. Best is trial 2 with value: 0.47841486010233536.\n",
            "[I 2025-11-06 20:26:13,938] Trial 4 finished with value: 0.6111380067448009 and parameters: {'max_depth': 10, 'learning_rate': 0.09266986678224005, 'subsample': 0.7876579314329203, 'colsample_bytree': 0.714300043954183, 'min_child_weight': 9, 'gamma': 0.5672740091775644, 'reg_alpha': 0.2863884048399181, 'reg_lambda': 0.8983968768884585, 'max_bin': 375, 'n_estimators': 718}. Best is trial 4 with value: 0.6111380067448009.\n",
            "[I 2025-11-06 20:28:50,242] Trial 5 finished with value: 0.284012369168905 and parameters: {'max_depth': 3, 'learning_rate': 0.02607579147267698, 'subsample': 0.6144016796248915, 'colsample_bytree': 0.8317939538345898, 'min_child_weight': 8, 'gamma': 0.433614335796782, 'reg_alpha': 0.8683448570262092, 'reg_lambda': 0.49322501662042273, 'max_bin': 196, 'n_estimators': 800}. Best is trial 4 with value: 0.6111380067448009.\n",
            "[I 2025-11-06 20:31:17,434] Trial 6 finished with value: 0.6192361626460581 and parameters: {'max_depth': 9, 'learning_rate': 0.19281966403730502, 'subsample': 0.9604770210178498, 'colsample_bytree': 0.970784778085195, 'min_child_weight': 5, 'gamma': 0.9492851827669881, 'reg_alpha': 0.917721003578699, 'reg_lambda': 0.8528839251290982, 'max_bin': 505, 'n_estimators': 533}. Best is trial 6 with value: 0.6192361626460581.\n",
            "[I 2025-11-06 20:34:31,838] Trial 7 finished with value: 0.4112427423518207 and parameters: {'max_depth': 4, 'learning_rate': 0.04584977941229104, 'subsample': 0.6365255785365872, 'colsample_bytree': 0.9303424012482523, 'min_child_weight': 8, 'gamma': 0.2796992717307111, 'reg_alpha': 0.3107096827412096, 'reg_lambda': 0.2576557755588206, 'max_bin': 211, 'n_estimators': 944}. Best is trial 6 with value: 0.6192361626460581.\n",
            "[I 2025-11-06 20:35:57,010] Trial 8 finished with value: 0.541432316414689 and parameters: {'max_depth': 6, 'learning_rate': 0.20584571005239272, 'subsample': 0.7461769516736572, 'colsample_bytree': 0.7388234528148799, 'min_child_weight': 9, 'gamma': 0.40930167989465216, 'reg_alpha': 0.40277340209199775, 'reg_lambda': 0.960766894584436, 'max_bin': 481, 'n_estimators': 356}. Best is trial 6 with value: 0.6192361626460581.\n",
            "[I 2025-11-06 20:39:19,197] Trial 9 finished with value: 0.4175170090115994 and parameters: {'max_depth': 4, 'learning_rate': 0.03643318836082151, 'subsample': 0.9693603685249057, 'colsample_bytree': 0.7746909853724894, 'min_child_weight': 5, 'gamma': 0.40179277345568576, 'reg_alpha': 0.23896531335639726, 'reg_lambda': 0.7512281841512125, 'max_bin': 449, 'n_estimators': 988}. Best is trial 6 with value: 0.6192361626460581.\n",
            "[I 2025-11-06 20:41:34,935] Trial 10 finished with value: 0.5882131430814166 and parameters: {'max_depth': 8, 'learning_rate': 0.27727571156410247, 'subsample': 0.866622450075062, 'colsample_bytree': 0.8821558884182086, 'min_child_weight': 1, 'gamma': 0.9804168364356154, 'reg_alpha': 0.003265344407208437, 'reg_lambda': 0.019985946522027953, 'max_bin': 507, 'n_estimators': 515}. Best is trial 6 with value: 0.6192361626460581.\n",
            "[I 2025-11-06 20:44:21,379] Trial 11 finished with value: 0.6041511776211366 and parameters: {'max_depth': 10, 'learning_rate': 0.09521390445396222, 'subsample': 0.8128434298029508, 'colsample_bytree': 0.662061777118784, 'min_child_weight': 6, 'gamma': 0.8369605623591788, 'reg_alpha': 0.6640267108337632, 'reg_lambda': 0.9726134060019176, 'max_bin': 415, 'n_estimators': 559}. Best is trial 6 with value: 0.6192361626460581.\n",
            "[I 2025-11-06 20:46:52,127] Trial 12 finished with value: 0.5830878332120515 and parameters: {'max_depth': 8, 'learning_rate': 0.08624420633350849, 'subsample': 0.8860184939199363, 'colsample_bytree': 0.999852220369608, 'min_child_weight': 6, 'gamma': 0.765651660906539, 'reg_alpha': 0.9892356149882618, 'reg_lambda': 0.853084070690663, 'max_bin': 285, 'n_estimators': 587}. Best is trial 6 with value: 0.6192361626460581.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost optimization completed!\n",
            "Best AUC-PR: 0.6192\n",
            "Best parameters:\n",
            "  max_depth: 9\n",
            "  learning_rate: 0.19281966403730502\n",
            "  subsample: 0.9604770210178498\n",
            "  colsample_bytree: 0.970784778085195\n",
            "  min_child_weight: 5\n",
            "  gamma: 0.9492851827669881\n",
            "  reg_alpha: 0.917721003578699\n",
            "  reg_lambda: 0.8528839251290982\n",
            "  max_bin: 505\n",
            "  n_estimators: 533\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# XGBoost hyperparameter optimization with enhanced features\n",
        "def objective_xgb_enhanced(trial):\n",
        "    # Define hyperparameter search space\n",
        "    params = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'tree_method': 'hist',\n",
        "        'predictor': 'gpu_predictor',\n",
        "        'eval_metric': 'aucpr',\n",
        "        'enable_categorical': True,\n",
        "        'verbosity': 0,\n",
        "\n",
        "        # Tunable parameters\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1.0),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1.0),\n",
        "        'max_bin': trial.suggest_int('max_bin', 128, 512),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "    }\n",
        "\n",
        "    # Calculate scale_pos_weight\n",
        "    pos = int((y_train == 1).sum())\n",
        "    neg = int((y_train == 0).sum())\n",
        "    params['scale_pos_weight'] = neg / max(pos, 1) if pos > 0 else 1.0\n",
        "\n",
        "    # Use enhanced features for tuning\n",
        "    dtrain = xgb.DMatrix(X_train_enhanced, label=y_train_np, enable_categorical=True)\n",
        "    dval = xgb.DMatrix(X_val_enhanced, label=y_val_np, enable_categorical=True)\n",
        "\n",
        "    # Train with early stopping\n",
        "    evals = [(dtrain, 'train'), (dval, 'val')]\n",
        "    model = xgb.train(\n",
        "        params, dtrain,\n",
        "        num_boost_round=params['n_estimators'],\n",
        "        evals=evals,\n",
        "        early_stopping_rounds=50,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "\n",
        "    # Predict on validation set\n",
        "    y_pred = model.predict(dval, iteration_range=(0, model.best_iteration + 1))\n",
        "\n",
        "    # Calculate AUC-PR (Area Under Precision-Recall Curve)\n",
        "    from sklearn.metrics import average_precision_score\n",
        "    score = average_precision_score(y_val_np, y_pred)\n",
        "\n",
        "    return score\n",
        "\n",
        "print(\"Starting XGBoost hyperparameter optimization...\")\n",
        "wandb_xgb_callback = WeightsAndBiasesCallback(\n",
        "    metric_name='auc_pr',\n",
        "    wandb_kwargs={**wandb_optuna_kwargs, 'job_type': 'xgboost_optuna'}\n",
        ")\n",
        "study_xgb = optuna.create_study(direction='maximize', study_name='xgb_enhanced')\n",
        "study_xgb.optimize(objective_xgb_enhanced, n_trials=50, timeout=1800, callbacks=[wandb_xgb_callback])  # 30 minutes timeout\n",
        "\n",
        "print(\"XGBoost optimization completed!\")\n",
        "print(f\"Best AUC-PR: {study_xgb.best_value:.4f}\")\n",
        "print(\"Best parameters:\")\n",
        "for key, value in study_xgb.best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Store best parameters for later use\n",
        "best_xgb_params = study_xgb.best_params.copy()\n",
        "best_xgb_params.update({\n",
        "    'objective': 'binary:logistic',\n",
        "    'tree_method': 'hist',\n",
        "    'predictor': 'gpu_predictor',\n",
        "    'eval_metric': 'aucpr',\n",
        "    'enable_categorical': True,\n",
        "})\n",
        "\n",
        "# Calculate scale_pos_weight for best model\n",
        "pos = int((y_train == 1).sum())\n",
        "neg = int((y_train == 0).sum())\n",
        "best_xgb_params['scale_pos_weight'] = neg / max(pos, 1) if pos > 0 else 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15067960",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "15067960",
        "outputId": "27723d9d-2cff-43ed-bb4b-ecd3571fd884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training optimized XGBoost model with enhanced features...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>auc_pr</td><td></td></tr><tr><td>colsample_bytree</td><td></td></tr><tr><td>gamma</td><td></td></tr><tr><td>learning_rate</td><td></td></tr><tr><td>max_bin</td><td></td></tr><tr><td>max_depth</td><td></td></tr><tr><td>min_child_weight</td><td></td></tr><tr><td>n_estimators</td><td></td></tr><tr><td>reg_alpha</td><td></td></tr><tr><td>reg_lambda</td><td></td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>auc_pr</td><td>0.58309</td></tr><tr><td>colsample_bytree</td><td>0.99985</td></tr><tr><td>gamma</td><td>0.76565</td></tr><tr><td>learning_rate</td><td>0.08624</td></tr><tr><td>max_bin</td><td>285</td></tr><tr><td>max_depth</td><td>8</td></tr><tr><td>min_child_weight</td><td>6</td></tr><tr><td>n_estimators</td><td>587</td></tr><tr><td>reg_alpha</td><td>0.98924</td></tr><tr><td>reg_lambda</td><td>0.85308</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sandy-smoke-13</strong> at: <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/1boex9th' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/1boex9th</a><br> View project at: <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251106_201633-1boex9th/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251106_204845-nwg2bmmw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/nwg2bmmw' target=\"_blank\">xgboost-optimized-final</a></strong> to <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/nwg2bmmw' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/nwg2bmmw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-aucpr:0.10736\tval-aucpr:0.10573\n",
            "[50]\ttrain-aucpr:0.56744\tval-aucpr:0.51666\n",
            "[100]\ttrain-aucpr:0.66665\tval-aucpr:0.57022\n",
            "[150]\ttrain-aucpr:0.72222\tval-aucpr:0.58628\n",
            "[200]\ttrain-aucpr:0.77047\tval-aucpr:0.59361\n",
            "[250]\ttrain-aucpr:0.81101\tval-aucpr:0.60063\n",
            "[300]\ttrain-aucpr:0.84113\tval-aucpr:0.60400\n",
            "[350]\ttrain-aucpr:0.86957\tval-aucpr:0.60933\n",
            "[400]\ttrain-aucpr:0.89293\tval-aucpr:0.61292\n",
            "[450]\ttrain-aucpr:0.91013\tval-aucpr:0.61570\n",
            "[500]\ttrain-aucpr:0.92460\tval-aucpr:0.61834\n",
            "[532]\ttrain-aucpr:0.93322\tval-aucpr:0.61892\n",
            "Best XGBoost iteration: 517\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>xgb/best_iteration</td><td></td></tr><tr><td>xgb/test_accuracy</td><td></td></tr><tr><td>xgb/test_auc_pr</td><td></td></tr><tr><td>xgb/test_auc_roc</td><td></td></tr><tr><td>xgb/test_balanced_acc</td><td></td></tr><tr><td>xgb/test_f1</td><td></td></tr><tr><td>xgb/test_precision</td><td></td></tr><tr><td>xgb/test_sensitivity</td><td></td></tr><tr><td>xgb/test_specificity</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>xgb/best_iteration</td><td>517</td></tr><tr><td>xgb/test_accuracy</td><td>0.97331</td></tr><tr><td>xgb/test_auc_pr</td><td>0.61949</td></tr><tr><td>xgb/test_auc_roc</td><td>0.97504</td></tr><tr><td>xgb/test_balanced_acc</td><td>0.87123</td></tr><tr><td>xgb/test_f1</td><td>0.46096</td></tr><tr><td>xgb/test_precision</td><td>0.32967</td></tr><tr><td>xgb/test_sensitivity</td><td>0.76602</td></tr><tr><td>xgb/test_specificity</td><td>0.97645</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">xgboost-optimized-final</strong> at: <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/nwg2bmmw' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/nwg2bmmw</a><br> View project at: <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251106_204845-nwg2bmmw/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimized XGBoost with Enhanced Features - Test Set Metrics:\n",
            "auc_roc: 0.9750\n",
            "auc_pr: 0.6195\n",
            "f1: 0.4610\n",
            "balanced_acc: 0.8712\n",
            "accuracy: 0.9733\n",
            "sensitivity: 0.7660\n",
            "specificity: 0.9764\n",
            "precision: 0.3297\n"
          ]
        }
      ],
      "source": [
        "# Train final XGBoost model with optimized parameters and enhanced features\n",
        "print(\"Training optimized XGBoost model with enhanced features...\")\n",
        "\n",
        "xgb_config = {}\n",
        "for key, value in best_xgb_params.items():\n",
        "    if isinstance(value, (np.floating, float)):\n",
        "        xgb_config[key] = float(value)\n",
        "    elif isinstance(value, (np.integer, int)):\n",
        "        xgb_config[key] = int(value)\n",
        "    else:\n",
        "        xgb_config[key] = value\n",
        "\n",
        "xgb_run = start_wandb_run(\n",
        "    run_name=\"xgboost-optimized-final\",\n",
        "    config=xgb_config,\n",
        "    job_type=\"xgboost_final\",\n",
        "    tags=['xgboost', 'final-model']\n",
        ")\n",
        "\n",
        "dtrain_enhanced = xgb.DMatrix(X_train_enhanced, label=y_train_np, enable_categorical=True)\n",
        "dval_enhanced = xgb.DMatrix(X_val_enhanced, label=y_val_np, enable_categorical=True)\n",
        "dtest_enhanced = xgb.DMatrix(X_test_enhanced, label=y_test_np, enable_categorical=True)\n",
        "\n",
        "evals = [(dtrain_enhanced, 'train'), (dval_enhanced, 'val')]\n",
        "best_xgb_model = xgb.train(\n",
        "    best_xgb_params, dtrain_enhanced,\n",
        "    num_boost_round=best_xgb_params['n_estimators'],\n",
        "    evals=evals,\n",
        "    early_stopping_rounds=50,\n",
        "    verbose_eval=50\n",
        ")\n",
        "\n",
        "print(f'Best XGBoost iteration: {best_xgb_model.best_iteration}')\n",
        "\n",
        "if xgb_run is not None:\n",
        "    wandb.log({'xgb/best_iteration': best_xgb_model.best_iteration})\n",
        "\n",
        "# Evaluate optimized XGBoost model\n",
        "y_pred_xgb_opt = best_xgb_model.predict(dtest_enhanced, iteration_range=(0, best_xgb_model.best_iteration + 1))\n",
        "y_true_xgb = y_test_np\n",
        "y_prob_xgb_opt = np.asarray(y_pred_xgb_opt)\n",
        "y_hat_xgb_opt = (y_prob_xgb_opt >= 0.5).astype(np.int32)\n",
        "\n",
        "# Calculate metrics for optimized XGBoost\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, balanced_accuracy_score, confusion_matrix, accuracy_score\n",
        "\n",
        "xgb_opt_metrics = {\n",
        "    'auc_roc': float(roc_auc_score(y_true_xgb, y_prob_xgb_opt)),\n",
        "    'auc_pr': float(average_precision_score(y_true_xgb, y_prob_xgb_opt)),\n",
        "    'f1': float(f1_score(y_true_xgb, y_hat_xgb_opt)),\n",
        "    'balanced_acc': float(balanced_accuracy_score(y_true_xgb, y_hat_xgb_opt)),\n",
        "    'accuracy': float(accuracy_score(y_true_xgb, y_hat_xgb_opt)),\n",
        "}\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_true_xgb, y_hat_xgb_opt).ravel()\n",
        "xgb_opt_metrics['sensitivity'] = float(tp / (tp + fn)) if (tp + fn) else 0.0\n",
        "xgb_opt_metrics['specificity'] = float(tn / (tn + fp)) if (tn + fp) else 0.0\n",
        "xgb_opt_metrics['precision'] = float(tp / (tp + fp)) if (tp + fp) else 0.0\n",
        "\n",
        "if xgb_run is not None:\n",
        "    wandb.log({f'xgb/test_{k}': v for k, v in xgb_opt_metrics.items()})\n",
        "    wandb.finish()\n",
        "\n",
        "print(\"\\nOptimized XGBoost with Enhanced Features - Test Set Metrics:\")\n",
        "for k, v in xgb_opt_metrics.items():\n",
        "    print(f'{k}: {v:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oaF_ppwmRsTW"
      },
      "id": "oaF_ppwmRsTW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Memory check**\n",
        "\n"
      ],
      "metadata": {
        "id": "ATZLAIMTSdxD"
      },
      "id": "ATZLAIMTSdxD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08951557"
      },
      "source": [
        "import psutil\n",
        "import cupy as cp\n",
        "import rmm\n",
        "import gc\n",
        "\n",
        "def check_memory_usage():\n",
        "    \"\"\"Prints current system RAM, GPU memory, and RMM pool usage.\"\"\"\n",
        "    print(\"--- Memory Usage Report ---\")\n",
        "\n",
        "    # System RAM\n",
        "    ram = psutil.virtual_memory()\n",
        "    print(f\"System RAM: {ram.used / (1024**3):.2f} GB used / {ram.total / (1024**3):.2f} GB total ({ram.percent:.1f}%)\")\n",
        "\n",
        "    # GPU Memory\n",
        "    try:\n",
        "        # Using cupy.cuda.Device().mem_info as an alternative\n",
        "        dev = cp.cuda.Device(0)\n",
        "        free_gpu_mem, total_gpu_mem = dev.mem_info\n",
        "        used_gpu_mem = total_gpu_mem - free_gpu_mem\n",
        "        print(f\"GPU Memory: {used_gpu_mem / (1024**3):.2f} GB used / {total_gpu_mem / (1024**3):.2f} GB total ({used_gpu_mem / total_gpu_mem * 100:.1f}%)\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve GPU memory info: {e}\")\n",
        "\n",
        "    # RMM Pool Usage (if initialized)\n",
        "    try:\n",
        "        if rmm.is_initialized():\n",
        "             # rmm.get_info() seems problematic, just report initialization status\n",
        "             print(\"RMM Pool: Initialized\")\n",
        "             # Optional: try to get pool size if needed, but avoid crashing\n",
        "             # pool_size_bytes = rmm.get_pool_size() # This might also fail depending on version\n",
        "             # print(f\"RMM Pool Size: {pool_size_bytes / (1024**3):.2f} GB\")\n",
        "        else:\n",
        "            print(\"RMM Pool: Not initialized\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve RMM pool info: {e}\")\n",
        "\n",
        "    print(\"-------------------------\")\n",
        "\n",
        "\n",
        "# Example usage (you can call this function at different points)\n",
        "# check_memory_usage()\n",
        "\n",
        "# Optional: Force garbage collection to potentially free up memory\n",
        "# gc.collect()\n",
        "# cp.cuda.runtime.deviceSynchronize() # Synchronize after GC if needed"
      ],
      "id": "08951557",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11417d31",
        "outputId": "3d84c70a-e708-43f2-ab72-919c9ddbbf61"
      },
      "source": [
        "check_memory_usage()"
      ],
      "id": "11417d31",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Memory Usage Report ---\n",
            "System RAM: 9.64 GB used / 167.05 GB total (6.7%)\n",
            "GPU Memory: 56.93 GB used / 79.32 GB total (71.8%)\n",
            "RMM Pool: Initialized\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f393a3a",
      "metadata": {
        "id": "1f393a3a"
      },
      "source": [
        "## Deep Learning Binary Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76179bd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76179bd6",
        "outputId": "2d078c01-b09b-4a7a-8118-de73469075dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced input dimension: 78\n",
            "Device: cuda\n",
            "Training samples: 2048576\n",
            "Validation samples: 438981\n",
            "Test samples: 438981\n"
          ]
        }
      ],
      "source": [
        "class DeepClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[512, 256, 128], dropout_rate=0.3, use_batch_norm=True):\n",
        "        super(DeepClassifier, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "            if use_batch_norm:\n",
        "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(prev_dim, 1))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# Prepare enhanced data for PyTorch\n",
        "X_train_enhanced_pt = torch.from_numpy(X_train_enhanced.astype('float32')).to(device)\n",
        "X_val_enhanced_pt = torch.from_numpy(X_val_enhanced.astype('float32')).to(device)\n",
        "X_test_enhanced_pt = torch.from_numpy(X_test_enhanced.astype('float32')).to(device)\n",
        "\n",
        "y_train_pt_device = torch.from_numpy(y_train_np.astype('float32')).unsqueeze(1).to(device)\n",
        "y_val_pt_device = torch.from_numpy(y_val_np.astype('float32')).unsqueeze(1).to(device)\n",
        "y_test_pt_device = torch.from_numpy(y_test_np.astype('float32')).unsqueeze(1).to(device)\n",
        "\n",
        "# Create enhanced data loaders\n",
        "train_dataset_enhanced = torch.utils.data.TensorDataset(X_train_enhanced_pt, y_train_pt_device)\n",
        "val_dataset_enhanced = torch.utils.data.TensorDataset(X_val_enhanced_pt, y_val_pt_device)\n",
        "test_dataset_enhanced = torch.utils.data.TensorDataset(X_test_enhanced_pt, y_test_pt_device)\n",
        "\n",
        "batch_size_dl = 512\n",
        "train_loader_enhanced = torch.utils.data.DataLoader(train_dataset_enhanced, batch_size=batch_size_dl, shuffle=True)\n",
        "val_loader_enhanced = torch.utils.data.DataLoader(val_dataset_enhanced, batch_size=batch_size_dl, shuffle=False)\n",
        "test_loader_enhanced = torch.utils.data.DataLoader(test_dataset_enhanced, batch_size=batch_size_dl, shuffle=False)\n",
        "\n",
        "enhanced_input_dim = X_train_enhanced.shape[1]\n",
        "print(f\"Enhanced input dimension: {enhanced_input_dim}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Training samples: {len(train_dataset_enhanced)}\")\n",
        "print(f\"Validation samples: {len(val_dataset_enhanced)}\")\n",
        "print(f\"Test samples: {len(test_dataset_enhanced)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3fbdedf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "d3fbdedf",
        "outputId": "008a1731-9a24-4631-e338-73f6058f83c9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Neural Network hyperparameter optimization...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3964846086.py:119: ExperimentalWarning: WeightsAndBiasesCallback is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  wandb_nn_callback = WeightsAndBiasesCallback(\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251106_210735-4maw7ebv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/4maw7ebv' target=\"_blank\">confused-capybara-15</a></strong> to <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/4maw7ebv' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/4maw7ebv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-06 21:08:56,909] A new study created in memory with name: neural_net_enhanced\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Train Loss: 1.3284, Val AUC-PR: 0.0156\n",
            "Epoch 20, Train Loss: 1.3210, Val AUC-PR: 0.0165\n",
            "Epoch 40, Train Loss: 1.3197, Val AUC-PR: 0.0177\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-06 21:39:21,695] Trial 0 finished with value: 0.018097182060470795 and parameters: {'n_layers': 4, 'first_layer': 640, 'layer_1': 256, 'layer_2': 96, 'layer_3': 64, 'dropout_rate': 0.4231534588942806, 'learning_rate': 0.00034925668984293033, 'use_batch_norm': True}. Best is trial 0 with value: 0.018097182060470795.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping at epoch 54\n",
            "Epoch 0, Train Loss: 115653.5815, Val AUC-PR: 0.0149\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-06 21:44:36,304] Trial 1 finished with value: 0.014895861096493926 and parameters: {'n_layers': 3, 'first_layer': 640, 'layer_1': 192, 'layer_2': 96, 'dropout_rate': 0.20067659031191273, 'learning_rate': 0.0008915978149024012, 'use_batch_norm': False}. Best is trial 0 with value: 0.018097182060470795.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping at epoch 10\n",
            "Epoch 0, Train Loss: 208208.2744, Val AUC-PR: 0.0149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-06 21:49:49,666] Trial 2 finished with value: 0.014895861096493926 and parameters: {'n_layers': 3, 'first_layer': 640, 'layer_1': 192, 'layer_2': 96, 'dropout_rate': 0.11991382857247257, 'learning_rate': 0.0018670828272981295, 'use_batch_norm': False}. Best is trial 0 with value: 0.018097182060470795.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 10\n",
            "Epoch 0, Train Loss: 1.3258, Val AUC-PR: 0.0157\n",
            "Epoch 20, Train Loss: 1.3219, Val AUC-PR: 0.0163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-06 22:07:01,441] Trial 3 finished with value: 0.017129497208836143 and parameters: {'n_layers': 3, 'first_layer': 896, 'layer_1': 256, 'layer_2': 64, 'dropout_rate': 0.4167494691705729, 'learning_rate': 0.0001090906563531294, 'use_batch_norm': True}. Best is trial 0 with value: 0.018097182060470795.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 32\n",
            "Epoch 0, Train Loss: 87725.2568, Val AUC-PR: 0.0149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-06 22:12:16,251] Trial 4 finished with value: 0.014895928962433283 and parameters: {'n_layers': 3, 'first_layer': 256, 'layer_1': 64, 'layer_2': 64, 'dropout_rate': 0.1061415265282117, 'learning_rate': 0.0011619838882547492, 'use_batch_norm': False}. Best is trial 0 with value: 0.018097182060470795.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 10\n",
            "Neural Network optimization completed!\n",
            "Best AUC-PR: 0.0181\n",
            "Best parameters:\n",
            "  n_layers: 4\n",
            "  first_layer: 640\n",
            "  layer_1: 256\n",
            "  layer_2: 96\n",
            "  layer_3: 64\n",
            "  dropout_rate: 0.4231534588942806\n",
            "  learning_rate: 0.00034925668984293033\n",
            "  use_batch_norm: True\n"
          ]
        }
      ],
      "source": [
        "# Neural Network hyperparameter optimization with Optuna\n",
        "def train_neural_net(model, train_loader, val_loader, epochs=100, lr=0.001, patience=15, wandb_run=None):\n",
        "    # Calculate class weights for imbalanced data\n",
        "    pos_count = (y_train_np == 1).sum()\n",
        "    neg_count = (y_train_np == 0).sum()\n",
        "    pos_weight = torch.tensor([neg_count / max(pos_count, 1)]).to(device)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
        "\n",
        "    best_val_score = 0.0\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    val_scores = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_targets = []\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                outputs = model(batch_x)\n",
        "                probs = torch.sigmoid(outputs)\n",
        "                val_preds.extend(probs.cpu().numpy())\n",
        "                val_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "        val_preds = np.array(val_preds).flatten()\n",
        "        val_targets = np.array(val_targets).flatten()\n",
        "        val_score = average_precision_score(val_targets, val_preds)\n",
        "\n",
        "        train_loss /= max(len(train_loader), 1)\n",
        "        train_losses.append(train_loss)\n",
        "        val_scores.append(val_score)\n",
        "\n",
        "        scheduler.step(val_score)\n",
        "\n",
        "        if wandb_run is not None:\n",
        "            wandb.log({\n",
        "                'nn/train_loss': train_loss,\n",
        "                'nn/val_auc_pr': val_score,\n",
        "                'nn/epoch': epoch\n",
        "            })\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val AUC-PR: {val_score:.4f}')\n",
        "\n",
        "        # Early stopping\n",
        "        if val_score > best_val_score:\n",
        "            best_val_score = val_score\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), output_dir / 'best_neural_net.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch}')\n",
        "                break\n",
        "\n",
        "    if wandb_run is not None:\n",
        "        wandb.log({'nn/best_val_auc_pr': best_val_score})\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(output_dir / 'best_neural_net.pth'))\n",
        "    return best_val_score, train_losses, val_scores\n",
        "\n",
        "def objective_neural_net(trial):\n",
        "    # Define hyperparameter search space\n",
        "    n_layers = trial.suggest_int('n_layers', 2, 5)\n",
        "    hidden_dims = []\n",
        "\n",
        "    # Start with a larger first layer and decrease\n",
        "    first_layer = trial.suggest_int('first_layer', 256, 1024, step=128)\n",
        "    hidden_dims.append(first_layer)\n",
        "\n",
        "    current_dim = first_layer\n",
        "    for i in range(1, n_layers):\n",
        "        # Each subsequent layer should be smaller\n",
        "        max_dim = max(64, current_dim // 2)\n",
        "        layer_dim = trial.suggest_int(f'layer_{i}', 64, max_dim, step=32)\n",
        "        hidden_dims.append(layer_dim)\n",
        "        current_dim = layer_dim\n",
        "\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "    use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])\n",
        "\n",
        "    # Create model\n",
        "    model = DeepClassifier(\n",
        "        input_dim=enhanced_input_dim,\n",
        "        hidden_dims=hidden_dims,\n",
        "        dropout_rate=dropout_rate,\n",
        "        use_batch_norm=use_batch_norm\n",
        "    ).to(device)\n",
        "\n",
        "    # Train model\n",
        "    try:\n",
        "        val_score, _, _ = train_neural_net(\n",
        "            model, train_loader_enhanced, val_loader_enhanced,\n",
        "            epochs=100, lr=learning_rate, patience=10\n",
        "        )\n",
        "        return val_score\n",
        "    except Exception as e:\n",
        "        print(f\"Trial failed: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "print(\"Starting Neural Network hyperparameter optimization...\")\n",
        "wandb_nn_callback = WeightsAndBiasesCallback(\n",
        "    metric_name='auc_pr',\n",
        "    wandb_kwargs={**wandb_optuna_kwargs, 'job_type': 'nn_optuna'}\n",
        ")\n",
        "study_nn = optuna.create_study(direction='maximize', study_name='neural_net_enhanced')\n",
        "study_nn.optimize(objective_neural_net, n_trials=30, timeout=3600, callbacks=[wandb_nn_callback])  # 1 hour timeout\n",
        "\n",
        "print(\"Neural Network optimization completed!\")\n",
        "print(f\"Best AUC-PR: {study_nn.best_value:.4f}\")\n",
        "print(\"Best parameters:\")\n",
        "for key, value in study_nn.best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Store best parameters\n",
        "best_nn_params = study_nn.best_params.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d8cdfc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e2d8cdfc",
        "outputId": "1bdb7125-6429-49b1-af11-e2ce45e54980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training optimized Neural Network...\n",
            "Final Neural Network Architecture:\n",
            "Input dim: 78\n",
            "Hidden layers: [640, 256, 96, 64]\n",
            "Dropout rate: 0.4231534588942806\n",
            "Batch normalization: True\n",
            "DeepClassifier(\n",
            "  (network): Sequential(\n",
            "    (0): Linear(in_features=78, out_features=640, bias=True)\n",
            "    (1): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.4231534588942806, inplace=False)\n",
            "    (4): Linear(in_features=640, out_features=256, bias=True)\n",
            "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): Dropout(p=0.4231534588942806, inplace=False)\n",
            "    (8): Linear(in_features=256, out_features=96, bias=True)\n",
            "    (9): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU()\n",
            "    (11): Dropout(p=0.4231534588942806, inplace=False)\n",
            "    (12): Linear(in_features=96, out_features=64, bias=True)\n",
            "    (13): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): ReLU()\n",
            "    (15): Dropout(p=0.4231534588942806, inplace=False)\n",
            "    (16): Linear(in_features=64, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>auc_pr</td><td></td></tr><tr><td>dropout_rate</td><td></td></tr><tr><td>first_layer</td><td></td></tr><tr><td>layer_1</td><td></td></tr><tr><td>layer_2</td><td></td></tr><tr><td>layer_3</td><td></td></tr><tr><td>learning_rate</td><td></td></tr><tr><td>n_layers</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>auc_pr</td><td>0.0149</td></tr><tr><td>dropout_rate</td><td>0.10614</td></tr><tr><td>first_layer</td><td>256</td></tr><tr><td>layer_1</td><td>64</td></tr><tr><td>layer_2</td><td>64</td></tr><tr><td>layer_3</td><td>64</td></tr><tr><td>learning_rate</td><td>0.00116</td></tr><tr><td>n_layers</td><td>3</td></tr><tr><td>use_batch_norm</td><td>False</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">confused-capybara-15</strong> at: <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/4maw7ebv' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/4maw7ebv</a><br> View project at: <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251106_210735-4maw7ebv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251106_222317-uoknck1m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/uoknck1m' target=\"_blank\">neural-network-final</a></strong> to <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/uoknck1m' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/uoknck1m</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Train Loss: 1.3265, Val AUC-PR: 0.0157\n",
            "Epoch 20, Train Loss: 1.3225, Val AUC-PR: 0.0169\n",
            "Epoch 40, Train Loss: 1.3201, Val AUC-PR: 0.0177\n",
            "Early stopping at epoch 59\n",
            "Final Neural Network training completed with validation AUC-PR: 0.0180\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>nn/best_val_auc_pr</td><td></td></tr><tr><td>nn/epoch</td><td></td></tr><tr><td>nn/test_accuracy</td><td></td></tr><tr><td>nn/test_auc_pr</td><td></td></tr><tr><td>nn/test_auc_roc</td><td></td></tr><tr><td>nn/test_balanced_acc</td><td></td></tr><tr><td>nn/test_f1</td><td></td></tr><tr><td>nn/test_precision</td><td></td></tr><tr><td>nn/test_sensitivity</td><td></td></tr><tr><td>nn/test_specificity</td><td></td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>nn/best_val_auc_pr</td><td>0.01803</td></tr><tr><td>nn/epoch</td><td>59</td></tr><tr><td>nn/test_accuracy</td><td>0.98511</td></tr><tr><td>nn/test_auc_pr</td><td>0.01858</td></tr><tr><td>nn/test_auc_roc</td><td>0.60308</td></tr><tr><td>nn/test_balanced_acc</td><td>0.50008</td></tr><tr><td>nn/test_f1</td><td>0.00031</td></tr><tr><td>nn/test_precision</td><td>1</td></tr><tr><td>nn/test_sensitivity</td><td>0.00015</td></tr><tr><td>nn/test_specificity</td><td>1</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">neural-network-final</strong> at: <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/uoknck1m' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/uoknck1m</a><br> View project at: <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251106_222317-uoknck1m/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimized Neural Network with Enhanced Features - Test Set Metrics:\n",
            "auc_roc: 0.6031\n",
            "auc_pr: 0.0186\n",
            "f1: 0.0003\n",
            "balanced_acc: 0.5001\n",
            "accuracy: 0.9851\n",
            "sensitivity: 0.0002\n",
            "specificity: 1.0000\n",
            "precision: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Train final neural network with optimized parameters\n",
        "print(\"Training optimized Neural Network...\")\n",
        "\n",
        "# Build hidden dimensions from best parameters\n",
        "best_hidden_dims = []\n",
        "best_hidden_dims.append(best_nn_params['first_layer'])\n",
        "for i in range(1, best_nn_params['n_layers']):\n",
        "    layer_key = f'layer_{i}'\n",
        "    if layer_key in best_nn_params:\n",
        "        best_hidden_dims.append(best_nn_params[layer_key])\n",
        "\n",
        "# Create final model with best parameters\n",
        "final_nn_model = DeepClassifier(\n",
        "    input_dim=enhanced_input_dim,\n",
        "    hidden_dims=best_hidden_dims,\n",
        "    dropout_rate=best_nn_params['dropout_rate'],\n",
        "    use_batch_norm=best_nn_params['use_batch_norm']\n",
        ").to(device)\n",
        "\n",
        "print(f\"Final Neural Network Architecture:\")\n",
        "print(f\"Input dim: {enhanced_input_dim}\")\n",
        "print(f\"Hidden layers: {best_hidden_dims}\")\n",
        "print(f\"Dropout rate: {best_nn_params['dropout_rate']}\")\n",
        "print(f\"Batch normalization: {best_nn_params['use_batch_norm']}\")\n",
        "print(final_nn_model)\n",
        "\n",
        "nn_run = start_wandb_run(\n",
        "    run_name=\"neural-network-final\",\n",
        "    config={\n",
        "        'architecture': best_hidden_dims,\n",
        "        'dropout_rate': best_nn_params['dropout_rate'],\n",
        "        'use_batch_norm': best_nn_params['use_batch_norm'],\n",
        "        'learning_rate': best_nn_params['learning_rate'] if 'learning_rate' in best_nn_params else None,\n",
        "        'batch_size': batch_size_dl\n",
        "    },\n",
        "    job_type=\"nn_final\",\n",
        "    tags=['neural-network', 'final-model']\n",
        ")\n",
        "\n",
        "final_val_score, train_losses_final, val_scores_final = train_neural_net(\n",
        "    final_nn_model, train_loader_enhanced, val_loader_enhanced,\n",
        "    epochs=200, lr=best_nn_params['learning_rate'], patience=20,\n",
        "    wandb_run=nn_run\n",
        ")\n",
        "\n",
        "print(f\"Final Neural Network training completed with validation AUC-PR: {final_val_score:.4f}\")\n",
        "\n",
        "# Evaluate final neural network on test set\n",
        "final_nn_model.eval()\n",
        "test_preds = []\n",
        "test_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_x, batch_y in test_loader_enhanced:\n",
        "        outputs = final_nn_model(batch_x)\n",
        "        probs = torch.sigmoid(outputs)\n",
        "        test_preds.extend(probs.cpu().numpy())\n",
        "        test_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "test_preds = np.array(test_preds).flatten()\n",
        "test_targets = np.array(test_targets).flatten()\n",
        "test_preds_binary = (test_preds >= 0.5).astype(np.int32)\n",
        "\n",
        "# Calculate metrics for neural network\n",
        "nn_metrics = {\n",
        "    'auc_roc': float(roc_auc_score(test_targets, test_preds)),\n",
        "    'auc_pr': float(average_precision_score(test_targets, test_preds)),\n",
        "    'f1': float(f1_score(test_targets, test_preds_binary)),\n",
        "    'balanced_acc': float(balanced_accuracy_score(test_targets, test_preds_binary)),\n",
        "    'accuracy': float(accuracy_score(test_targets, test_preds_binary)),\n",
        "}\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(test_targets, test_preds_binary).ravel()\n",
        "nn_metrics['sensitivity'] = float(tp / (tp + fn)) if (tp + fn) else 0.0\n",
        "nn_metrics['specificity'] = float(tn / (tn + fp)) if (tn + fp) else 0.0\n",
        "nn_metrics['precision'] = float(tp / (tp + fp)) if (tp + fp) else 0.0\n",
        "\n",
        "if nn_run is not None:\n",
        "    wandb.log({f'nn/test_{k}': v for k, v in nn_metrics.items()})\n",
        "    wandb.finish()\n",
        "\n",
        "print(\"\\nOptimized Neural Network with Enhanced Features - Test Set Metrics:\")\n",
        "for k, v in nn_metrics.items():\n",
        "    print(f'{k}: {v:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "677a8b2c",
      "metadata": {
        "id": "677a8b2c"
      },
      "source": [
        "## FT-Transformer Binary Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cef8ff8",
      "metadata": {
        "id": "4cef8ff8"
      },
      "outputs": [],
      "source": [
        "class FeatureTokenizer(nn.Module):\n",
        "    \"\"\"Tokenizes continuous features into embeddings for FT-Transformer.\"\"\"\n",
        "\n",
        "    def __init__(self, n_features: int, d_token: int):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(n_features, d_token) * 0.02)\n",
        "        self.bias = nn.Parameter(torch.zeros(n_features, d_token))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (batch, n_features)\n",
        "        x = x.unsqueeze(-1)  # (batch, n_features, 1)\n",
        "        return x * self.weight + self.bias  # broadcast multiply\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_token: int, n_heads: int, dropout: float, ffn_d_hidden: int):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim=d_token,\n",
        "            num_heads=n_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_token)\n",
        "        self.norm2 = nn.LayerNorm(d_token)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_token, ffn_d_hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ffn_d_hidden, d_token)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        attn_output, _ = self.attn(x, x, x, need_weights=False)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))\n",
        "        return x\n",
        "\n",
        "\n",
        "class FTTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_features: int,\n",
        "        d_token: int = 192,\n",
        "        n_heads: int = 8,\n",
        "        n_layers: int = 4,\n",
        "        dropout: float = 0.2,\n",
        "        ffn_d_hidden: int = 384\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.tokenizer = FeatureTokenizer(n_features, d_token)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_token, n_heads, dropout, ffn_d_hidden)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_token)\n",
        "        self.head = nn.Linear(d_token, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        tokens = self.tokenizer(x)\n",
        "        batch_size = tokens.size(0)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat([cls_tokens, tokens], dim=1)\n",
        "        x = self.dropout(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        cls_representation = self.norm(x[:, 0, :])\n",
        "        logits = self.head(cls_representation)\n",
        "        return logits.squeeze(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a0d691f",
      "metadata": {
        "id": "4a0d691f"
      },
      "outputs": [],
      "source": [
        "def train_ft_transformer(\n",
        "    model: FTTransformer,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs: int = 150,\n",
        "    lr: float = 3e-4,\n",
        "    weight_decay: float = 1e-5,\n",
        "    patience: int = 20,\n",
        "    wandb_run=None\n",
        "):\n",
        "    pos_count = (y_train_np == 1).sum()\n",
        "    neg_count = (y_train_np == 0).sum()\n",
        "    pos_weight = torch.tensor([neg_count / max(pos_count, 1)]).to(device)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', patience=6, factor=0.5, min_lr=1e-5\n",
        "    )\n",
        "\n",
        "    best_val_score = 0.0\n",
        "    patience_counter = 0\n",
        "    history = {'train_loss': [], 'val_auc_pr': []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch_x)\n",
        "            loss = criterion(logits, batch_y.squeeze(1))\n",
        "            # Check for NaN loss during training\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"NaN loss detected at epoch {epoch}, batch {len(train_loader)-len(train_loader_enhanced) + 1}. Skipping backward pass for this batch.\")\n",
        "                continue # Skip this batch\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss /= max(len(train_loader), 1)\n",
        "\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_targets = []\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                logits = model(batch_x)\n",
        "                probs = torch.sigmoid(logits)\n",
        "                val_preds.extend(probs.detach().cpu().numpy())\n",
        "                val_targets.extend(batch_y.detach().cpu().numpy())\n",
        "\n",
        "        val_preds = np.array(val_preds).flatten()\n",
        "        val_targets = np.array(val_targets).flatten()\n",
        "\n",
        "        # Add checks for NaNs before calculating metrics\n",
        "        if np.isnan(val_preds).any():\n",
        "            print(f\"NaNs found in val_preds at epoch {epoch}. Number of NaNs: {np.isnan(val_preds).sum()}\")\n",
        "            # Option 1: Skip metric calculation for this epoch (and potentially trigger early stopping if patience runs out)\n",
        "            val_auc_pr = 0.0 # Or keep best_val_score, depending on desired behavior\n",
        "            if wandb_run is not None:\n",
        "                wandb.log({\n",
        "                    'ft/train_loss': epoch_loss,\n",
        "                    'ft/val_auc_pr': float('nan'), # Log NaN to Wandb\n",
        "                    'ft/epoch': epoch\n",
        "                })\n",
        "            history['train_loss'].append(epoch_loss)\n",
        "            history['val_auc_pr'].append(float('nan'))\n",
        "            scheduler.step(val_auc_pr) # Step with a low score\n",
        "            print(f\"Epoch {epoch}: Train Loss = {epoch_loss:.4f}, Val AUC-PR = NaN (NaNs detected)\")\n",
        "            # Increment patience counter if not improving (or always if NaNs are critical)\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch} due to persistent NaNs in validation predictions\")\n",
        "                break\n",
        "            continue # Skip the rest of the loop for this epoch\n",
        "\n",
        "        if np.isnan(val_targets).any():\n",
        "             print(f\"NaNs found in val_targets at epoch {epoch}. This indicates a data issue.\")\n",
        "             # This is unexpected if data prep was correct. Could raise an error or log and stop.\n",
        "             raise ValueError(\"NaNs found in validation targets!\")\n",
        "\n",
        "\n",
        "        val_auc_pr = average_precision_score(val_targets, val_preds)\n",
        "\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['val_auc_pr'].append(val_auc_pr)\n",
        "\n",
        "        scheduler.step(val_auc_pr)\n",
        "\n",
        "        if wandb_run is not None:\n",
        "            wandb.log({\n",
        "                'ft/train_loss': epoch_loss,\n",
        "                'ft/val_auc_pr': val_auc_pr,\n",
        "                'ft/epoch': epoch\n",
        "            })\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch {epoch}: Train Loss = {epoch_loss:.4f}, Val AUC-PR = {val_auc_pr:.4f}\")\n",
        "\n",
        "        if val_auc_pr > best_val_score:\n",
        "            best_val_score = val_auc_pr\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), output_dir / 'best_ft_transformer.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # Load best model if a valid score was ever achieved\n",
        "    if best_val_score > 0: # Assuming AUC-PR > 0 is a valid score\n",
        "        model.load_state_dict(torch.load(output_dir / 'best_ft_transformer.pth'))\n",
        "    else:\n",
        "        print(\"Warning: No valid validation AUC-PR achieved. Model state not loaded from checkpoint.\")\n",
        "\n",
        "\n",
        "    return best_val_score, history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NaNs and Infs in the NumPy arrays after preprocessing\n",
        "print(\"Checking X_train_np for NaNs:\", np.isnan(X_train_np).any())\n",
        "print(\"Checking X_train_np for Infs:\", np.isinf(X_train_np).any())\n",
        "print(\"Checking y_train_np for NaNs:\", np.isnan(y_train_np).any())\n",
        "print(\"Checking y_train_np for Infs:\", np.isinf(y_train_np).any())\n",
        "\n",
        "# Optionally, check the PyTorch tensors as well\n",
        "# print(\"Checking X_train_pt for NaNs:\", torch.isnan(X_train_pt).any())\n",
        "# print(\"Checking X_train_pt for Infs:\", torch.isinf(X_train_pt).any())\n",
        "# print(\"Checking y_train_pt for NaNs:\", torch.isnan(y_train_pt).any())\n",
        "# print(\"Checking y_train_pt for Infs:\", torch.isinf(y_train_pt).any())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEfUEpo3kNGn",
        "outputId": "00d45550-ebf1-4414-96e0-91d4a0609b17"
      },
      "id": "qEfUEpo3kNGn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking X_train_np for NaNs: False\n",
            "Checking X_train_np for Infs: False\n",
            "Checking y_train_np for NaNs: False\n",
            "Checking y_train_np for Infs: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29751515",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "29751515",
        "outputId": "b57cf746-aef9-4fde-9914-4a03ddf881df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training FT-Transformer with enhanced features...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ft/epoch</td><td></td></tr><tr><td>ft/train_loss</td><td></td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ft/epoch</td><td>18</td></tr><tr><td>ft/train_loss</td><td>0</td></tr><tr><td>ft/val_auc_pr</td><td>nan</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ft-transformer-final</strong> at: <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/3m5eeu34' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/3m5eeu34</a><br> View project at: <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251106_231554-3m5eeu34/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251106_235105-g5e8r6kw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/g5e8r6kw' target=\"_blank\">ft-transformer-final</a></strong> to <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/g5e8r6kw' target=\"_blank\">https://wandb.ai/segettii-segetii/amttp-rapids-gpu/runs/g5e8r6kw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 1, batch 1. Skipping backward pass for this batch.\n",
            "NaNs found in val_preds at epoch 1. Number of NaNs: 438981\n",
            "Epoch 1: Train Loss = 0.0004, Val AUC-PR = NaN (NaNs detected)\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n",
            "NaN loss detected at epoch 2, batch 1. Skipping backward pass for this batch.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1939415414.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m ft_best_val_auc, ft_history = train_ft_transformer(\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mft_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtrain_loader_enhanced\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2476285941.py\u001b[0m in \u001b[0;36mtrain_ft_transformer\u001b[0;34m(model, train_loader, val_loader, epochs, lr, weight_decay, patience, wandb_run)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Check for NaN loss during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"NaN loss detected at epoch {epoch}, batch {len(train_loader)-len(train_loader_enhanced) + 1}. Skipping backward pass for this batch.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0;32mcontinue\u001b[0m \u001b[0;31m# Skip this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"Training FT-Transformer with enhanced features...\")\n",
        "ft_config = {\n",
        "    'd_token': 192,\n",
        "    'n_heads': 8,\n",
        "    'n_layers': 4,\n",
        "    'dropout': 0.2,\n",
        "    'ffn_d_hidden': 384,\n",
        "    'lr': 3e-4,\n",
        "    'weight_decay': 1e-5,\n",
        "    'epochs': 160,\n",
        "    'patience': 18\n",
        "}\n",
        "\n",
        "ft_model = FTTransformer(\n",
        "    n_features=enhanced_input_dim,\n",
        "    d_token=ft_config['d_token'],\n",
        "    n_heads=ft_config['n_heads'],\n",
        "    n_layers=ft_config['n_layers'],\n",
        "    dropout=ft_config['dropout'],\n",
        "    ffn_d_hidden=ft_config['ffn_d_hidden']\n",
        ").to(device)\n",
        "\n",
        "ft_run = start_wandb_run(\n",
        "    run_name=\"ft-transformer-final\",\n",
        "    config=ft_config,\n",
        "    job_type=\"ft_transformer\",\n",
        "    tags=['ft-transformer', 'final-model']\n",
        ")\n",
        "\n",
        "ft_best_val_auc, ft_history = train_ft_transformer(\n",
        "    ft_model,\n",
        "    train_loader_enhanced,\n",
        "    val_loader_enhanced,\n",
        "    epochs=ft_config['epochs'],\n",
        "    lr=ft_config['lr'],\n",
        "    weight_decay=ft_config['weight_decay'],\n",
        "    patience=ft_config['patience'],\n",
        "    wandb_run=ft_run\n",
        ")\n",
        "\n",
        "print(f\"Best validation AUC-PR: {ft_best_val_auc:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "ft_model.eval()\n",
        "ft_test_probs = []\n",
        "ft_test_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_x, batch_y in test_loader_enhanced:\n",
        "        logits = ft_model(batch_x)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        ft_test_probs.extend(probs.cpu().numpy())\n",
        "        ft_test_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "ft_test_probs = np.array(ft_test_probs).flatten()\n",
        "ft_test_targets = np.array(ft_test_targets).flatten()\n",
        "ft_test_preds = (ft_test_probs >= 0.5).astype(np.int32)\n",
        "\n",
        "ft_metrics = {\n",
        "    'auc_roc': float(roc_auc_score(ft_test_targets, ft_test_probs)),\n",
        "    'auc_pr': float(average_precision_score(ft_test_targets, ft_test_probs)),\n",
        "    'f1': float(f1_score(ft_test_targets, ft_test_preds)),\n",
        "    'balanced_acc': float(balanced_accuracy_score(ft_test_targets, ft_test_preds)),\n",
        "    'accuracy': float(accuracy_score(ft_test_targets, ft_test_preds)),\n",
        "}\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(ft_test_targets, ft_test_preds).ravel()\n",
        "ft_metrics['sensitivity'] = float(tp / (tp + fn)) if (tp + fn) else 0.0\n",
        "ft_metrics['specificity'] = float(tn / (tn + fp)) if (tn + fp) else 0.0\n",
        "ft_metrics['precision'] = float(tp / (tp + fp)) if (tp + fp) else 0.0\n",
        "\n",
        "if ft_run is not None:\n",
        "    wandb.log({'ft/best_val_auc_pr': ft_best_val_auc})\n",
        "    wandb.log({f'ft/test_{k}': v for k, v in ft_metrics.items()})\n",
        "    wandb.finish()\n",
        "\n",
        "print(\"\\nFT-Transformer - Test Set Metrics:\")\n",
        "for k, v in ft_metrics.items():\n",
        "    print(f\"{k}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbc1af1e",
      "metadata": {
        "id": "fbc1af1e"
      },
      "source": [
        "## Model Explainability with SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a59a41",
      "metadata": {
        "id": "48a59a41"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create feature names for enhanced dataset\n",
        "original_feature_names = keep_cols\n",
        "autoencoder_feature_names = [f'ae_feature_{i}' for i in range(encoded_train.shape[1])]\n",
        "all_feature_names = original_feature_names + autoencoder_feature_names\n",
        "\n",
        "print(f\"Total features for SHAP analysis: {len(all_feature_names)}\")\n",
        "print(f\"Original features: {len(original_feature_names)}\")\n",
        "print(f\"Autoencoder features: {len(autoencoder_feature_names)}\")\n",
        "\n",
        "# Sample data for SHAP analysis (to manage computational cost)\n",
        "n_shap_samples = min(1000, len(X_test_enhanced))\n",
        "shap_indices = np.random.choice(len(X_test_enhanced), n_shap_samples, replace=False)\n",
        "X_shap = X_test_enhanced[shap_indices]\n",
        "y_shap = y_test_np[shap_indices]\n",
        "\n",
        "print(f\"Using {n_shap_samples} samples for SHAP analysis\")\n",
        "\n",
        "# SHAP analysis for XGBoost\n",
        "print(\"\\nGenerating SHAP explanations for XGBoost...\")\n",
        "explainer_xgb = shap.TreeExplainer(best_xgb_model)\n",
        "shap_values_xgb = explainer_xgb.shap_values(X_shap)\n",
        "\n",
        "print(\"XGBoost SHAP values shape:\", shap_values_xgb.shape)\n",
        "\n",
        "# SHAP analysis for Neural Network\n",
        "print(\"\\nGenerating SHAP explanations for Neural Network...\")\n",
        "\n",
        "# Create a wrapper function for the neural network\n",
        "def nn_predict_wrapper(X):\n",
        "    final_nn_model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_tensor = torch.from_numpy(X.astype('float32')).to(device)\n",
        "        outputs = final_nn_model(X_tensor)\n",
        "        probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "    return probs.flatten()\n",
        "\n",
        "# Use a smaller background dataset for neural network SHAP\n",
        "background_size = min(100, len(X_train_enhanced))\n",
        "background_indices = np.random.choice(len(X_train_enhanced), background_size, replace=False)\n",
        "X_background = X_train_enhanced[background_indices]\n",
        "\n",
        "explainer_nn = shap.KernelExplainer(nn_predict_wrapper, X_background)\n",
        "shap_values_nn = explainer_nn.shap_values(X_shap[:100])  # Limit to 100 samples for NN due to computational cost\n",
        "\n",
        "print(\"Neural Network SHAP values shape:\", shap_values_nn.shape)\n",
        "\n",
        "# SHAP analysis for FT-Transformer\n",
        "print(\"\\nGenerating SHAP explanations for FT-Transformer...\")\n",
        "ft_model.eval()\n",
        "\n",
        "class _FTProbabilityWrapper(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.base_model(x)).unsqueeze(-1)\n",
        "\n",
        "ft_background_size = min(128, len(X_train_enhanced))\n",
        "ft_background_indices = np.random.choice(len(X_train_enhanced), ft_background_size, replace=False)\n",
        "X_background_ft = torch.from_numpy(X_train_enhanced[ft_background_indices].astype('float32')).to(device)\n",
        "X_shap_ft_tensor = torch.from_numpy(X_shap.astype('float32')).to(device)\n",
        "\n",
        "ft_wrapper = _FTProbabilityWrapper(ft_model).to(device)\n",
        "explainer_ft = shap.DeepExplainer(ft_wrapper, X_background_ft)\n",
        "ft_shap_values = explainer_ft.shap_values(X_shap_ft_tensor)\n",
        "\n",
        "if isinstance(ft_shap_values, list):\n",
        "    ft_shap_values = ft_shap_values[0]\n",
        "\n",
        "ft_shap_values = np.array(ft_shap_values)\n",
        "print(\"FT-Transformer SHAP values shape:\", ft_shap_values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a38063",
      "metadata": {
        "id": "61a38063"
      },
      "outputs": [],
      "source": [
        "# Create SHAP visualizations\n",
        "plt.style.use('default')\n",
        "fig, axes = plt.subplots(3, 2, figsize=(20, 24))\n",
        "\n",
        "# XGBoost SHAP Summary Plot\n",
        "plt.sca(axes[0, 0])\n",
        "shap.summary_plot(shap_values_xgb, X_shap, feature_names=all_feature_names,\n",
        "                  max_display=20, show=False)\n",
        "plt.title('XGBoost SHAP Summary Plot (Top 20 Features)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# XGBoost SHAP Bar Plot\n",
        "plt.sca(axes[0, 1])\n",
        "shap.summary_plot(shap_values_xgb, X_shap, feature_names=all_feature_names,\n",
        "                  plot_type=\"bar\", max_display=15, show=False)\n",
        "plt.title('XGBoost Feature Importance (SHAP)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Neural Network SHAP Summary Plot\n",
        "plt.sca(axes[1, 0])\n",
        "shap.summary_plot(shap_values_nn, X_shap[:100], feature_names=all_feature_names,\n",
        "                  max_display=20, show=False)\n",
        "plt.title('Neural Network SHAP Summary Plot (Top 20 Features)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Neural Network SHAP Bar Plot\n",
        "plt.sca(axes[1, 1])\n",
        "shap.summary_plot(shap_values_nn, X_shap[:100], feature_names=all_feature_names,\n",
        "                  plot_type=\"bar\", max_display=15, show=False)\n",
        "plt.title('Neural Network Feature Importance (SHAP)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# FT-Transformer SHAP Summary Plot\n",
        "plt.sca(axes[2, 0])\n",
        "shap.summary_plot(ft_shap_values, X_shap, feature_names=all_feature_names,\n",
        "                  max_display=20, show=False)\n",
        "plt.title('FT-Transformer SHAP Summary Plot (Top 20 Features)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# FT-Transformer SHAP Bar Plot\n",
        "plt.sca(axes[2, 1])\n",
        "shap.summary_plot(ft_shap_values, X_shap, feature_names=all_feature_names,\n",
        "                  plot_type=\"bar\", max_display=15, show=False)\n",
        "plt.title('FT-Transformer Feature Importance (SHAP)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / 'shap_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Feature importance comparison\n",
        "def get_top_features(shap_values, feature_names, top_n=10):\n",
        "    mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
        "    top_indices = np.argsort(mean_abs_shap)[-top_n:][::-1]\n",
        "    return [(feature_names[i], mean_abs_shap[i]) for i in top_indices]\n",
        "\n",
        "xgb_top_features = get_top_features(shap_values_xgb, all_feature_names, 15)\n",
        "nn_top_features = get_top_features(shap_values_nn, all_feature_names, 15)\n",
        "ft_top_features = get_top_features(ft_shap_values, all_feature_names, 15)\n",
        "\n",
        "print(\"\\nTop 15 Most Important Features (by mean |SHAP value|):\")\n",
        "print(\"\\nXGBoost:\")\n",
        "for i, (feature, importance) in enumerate(xgb_top_features, 1):\n",
        "    print(f\"{i:2d}. {feature}: {importance:.4f}\")\n",
        "\n",
        "print(\"\\nNeural Network:\")\n",
        "for i, (feature, importance) in enumerate(nn_top_features, 1):\n",
        "    print(f\"{i:2d}. {feature}: {importance:.4f}\")\n",
        "\n",
        "print(\"\\nFT-Transformer:\")\n",
        "for i, (feature, importance) in enumerate(ft_top_features, 1):\n",
        "    print(f\"{i:2d}. {feature}: {importance:.4f}\")\n",
        "\n",
        "# Save SHAP values for later analysis\n",
        "np.save(output_dir / 'shap_values_xgb.npy', shap_values_xgb)\n",
        "np.save(output_dir / 'shap_values_nn.npy', shap_values_nn)\n",
        "np.save(output_dir / 'shap_values_ft.npy', ft_shap_values)\n",
        "np.save(output_dir / 'shap_test_data.npy', X_shap)\n",
        "\n",
        "print(f\"\\nSHAP analysis completed and saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87820a57",
      "metadata": {
        "id": "87820a57"
      },
      "source": [
        "## Model Comparison and Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a5369a8",
      "metadata": {
        "id": "3a5369a8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Compare all models: Original XGBoost, Optimized XGBoost, Neural Network, FT-Transformer\n",
        "model_comparison = pd.DataFrame({\n",
        "    'Model': [\n",
        "        'Original XGBoost',\n",
        "        'Optimized XGBoost + Enhanced Features',\n",
        "        'Neural Network + Enhanced Features',\n",
        "        'FT-Transformer + Enhanced Features'\n",
        "    ],\n",
        "    'AUC-ROC': [\n",
        "        metrics['auc_roc'],\n",
        "        xgb_opt_metrics['auc_roc'],\n",
        "        nn_metrics['auc_roc'],\n",
        "        ft_metrics['auc_roc']\n",
        "    ],\n",
        "    'AUC-PR': [\n",
        "        metrics['auc_pr'],\n",
        "        xgb_opt_metrics['auc_pr'],\n",
        "        nn_metrics['auc_pr'],\n",
        "        ft_metrics['auc_pr']\n",
        "    ],\n",
        "    'F1-Score': [\n",
        "        metrics['f1'],\n",
        "        xgb_opt_metrics['f1'],\n",
        "        nn_metrics['f1'],\n",
        "        ft_metrics['f1']\n",
        "    ],\n",
        "    'Balanced Accuracy': [\n",
        "        metrics['balanced_acc'],\n",
        "        xgb_opt_metrics['balanced_acc'],\n",
        "        nn_metrics['balanced_acc'],\n",
        "        ft_metrics['balanced_acc']\n",
        "    ],\n",
        "    'Accuracy': [\n",
        "        metrics['accuracy'],\n",
        "        xgb_opt_metrics['accuracy'],\n",
        "        nn_metrics['accuracy'],\n",
        "        ft_metrics['accuracy']\n",
        "    ],\n",
        "    'Sensitivity': [\n",
        "        metrics['sensitivity'],\n",
        "        xgb_opt_metrics['sensitivity'],\n",
        "        nn_metrics['sensitivity'],\n",
        "        ft_metrics['sensitivity']\n",
        "    ],\n",
        "    'Specificity': [\n",
        "        metrics['specificity'],\n",
        "        xgb_opt_metrics['specificity'],\n",
        "        nn_metrics['specificity'],\n",
        "        ft_metrics['specificity']\n",
        "    ],\n",
        "    'Precision': [\n",
        "        metrics['precision'],\n",
        "        xgb_opt_metrics['precision'],\n",
        "        nn_metrics['precision'],\n",
        "        ft_metrics['precision']\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"Model Performance Comparison:\")\n",
        "print(\"=\" * 80)\n",
        "print(model_comparison.round(4).to_string(index=False))\n",
        "\n",
        "# Calculate improvements over baseline for each advanced model\n",
        "print(\"\\n\\nPerformance Improvements over Original XGBoost:\")\n",
        "print(\"=\" * 60)\n",
        "for i, model_name in enumerate(model_comparison['Model'][1:], 1):\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    for metric in ['AUC-ROC', 'AUC-PR', 'F1-Score', 'Balanced Accuracy']:\n",
        "        original = model_comparison.loc[0, metric]\n",
        "        improved = model_comparison.loc[i, metric]\n",
        "        improvement = ((improved - original) / max(original, 1e-8)) * 100\n",
        "        print(f\"  {metric}: {improvement:+.2f}% ({original:.4f}  {improved:.4f})\")\n",
        "\n",
        "# Find best model for each metric\n",
        "print(\"\\n\\nBest Model per Metric:\")\n",
        "print(\"=\" * 30)\n",
        "for metric in ['AUC-ROC', 'AUC-PR', 'F1-Score', 'Balanced Accuracy', 'Accuracy']:\n",
        "    best_idx = model_comparison[metric].idxmax()\n",
        "    best_model = model_comparison.loc[best_idx, 'Model']\n",
        "    best_score = model_comparison.loc[best_idx, metric]\n",
        "    print(f\"{metric}: {best_model} ({best_score:.4f})\")\n",
        "\n",
        "# Create interactive comparison plot\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=('AUC-ROC Comparison', 'AUC-PR Comparison', 'F1-Score Comparison', 'Balanced Accuracy Comparison'),\n",
        "    specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
        "           [{'type': 'bar'}, {'type': 'bar'}]]\n",
        ")\n",
        "\n",
        "metrics_to_plot = ['AUC-ROC', 'AUC-PR', 'F1-Score', 'Balanced Accuracy']\n",
        "colors = ['lightblue', 'lightgreen', 'lightcoral', 'khaki']\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    row = (i // 2) + 1\n",
        "    col = (i % 2) + 1\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Bar(\n",
        "            x=model_comparison['Model'],\n",
        "            y=model_comparison[metric],\n",
        "            name=metric,\n",
        "            marker_color=colors,\n",
        "            text=model_comparison[metric].round(4),\n",
        "            textposition='auto',\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=\"Model Performance Comparison\",\n",
        "    title_x=0.5,\n",
        "    height=800,\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig.update_yaxes(range=[0, 1])\n",
        "fig.show()\n",
        "\n",
        "# Save comparison results\n",
        "model_comparison.to_csv(output_dir / 'model_comparison.csv', index=False)\n",
        "print(f\"\\nModel comparison saved to {output_dir / 'model_comparison.csv'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7ace637",
      "metadata": {
        "id": "b7ace637"
      },
      "outputs": [],
      "source": [
        "# Enhanced artifact saving with all models and results\n",
        "import json\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "# Create comprehensive metadata\n",
        "metadata = {\n",
        "    'experiment_info': {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'dataset': dataset_path,\n",
        "        'total_samples': len(X_train) + len(X_val) + len(X_test),\n",
        "        'features_original': n_features,\n",
        "        'features_enhanced': X_train_enhanced.shape[1],\n",
        "        'autoencoder_compression_ratio': n_features / encoding_dim,\n",
        "    },\n",
        "    'data_splits': {\n",
        "        'train_samples': len(X_train),\n",
        "        'val_samples': len(X_val),\n",
        "        'test_samples': len(X_test),\n",
        "        'train_pos_ratio': float(pos / (pos + neg)),\n",
        "    },\n",
        "    'models': {\n",
        "        'original_xgboost': {\n",
        "            'type': 'XGBoost',\n",
        "            'features': 'original',\n",
        "            'hyperparameter_tuning': False,\n",
        "            'metrics': metrics\n",
        "        },\n",
        "        'optimized_xgboost': {\n",
        "            'type': 'XGBoost',\n",
        "            'features': 'enhanced (original + autoencoder)',\n",
        "            'hyperparameter_tuning': True,\n",
        "            'best_params': best_xgb_params,\n",
        "            'optuna_trials': len(study_xgb.trials),\n",
        "            'metrics': xgb_opt_metrics\n",
        "        },\n",
        "        'neural_network': {\n",
        "            'type': 'Deep Neural Network',\n",
        "            'features': 'enhanced (original + autoencoder)',\n",
        "            'hyperparameter_tuning': True,\n",
        "            'best_params': best_nn_params,\n",
        "            'architecture': best_hidden_dims,\n",
        "            'optuna_trials': len(study_nn.trials),\n",
        "            'metrics': nn_metrics\n",
        "        },\n",
        "        'ft_transformer': {\n",
        "            'type': 'FT-Transformer',\n",
        "            'features': 'enhanced (original + autoencoder)',\n",
        "            'hyperparameter_tuning': False,\n",
        "            'config': ft_config,\n",
        "            'best_val_auc_pr': ft_best_val_auc,\n",
        "            'metrics': ft_metrics\n",
        "        }\n",
        "    },\n",
        "    'autoencoder': {\n",
        "        'input_dim': n_features,\n",
        "        'encoding_dim': encoding_dim,\n",
        "        'architecture': str(autoencoder),\n",
        "        'final_train_loss': train_losses[-1],\n",
        "        'final_val_loss': val_losses[-1]\n",
        "    },\n",
        "    'feature_info': {\n",
        "        'categorical_features': cat_cols,\n",
        "        'numerical_features': num_cols,\n",
        "        'categorical_mappings': cat_mappings,\n",
        "        'feature_names_enhanced': all_feature_names\n",
        "    },\n",
        "    'explainability': {\n",
        "        'shap_samples_analyzed': n_shap_samples,\n",
        "        'top_xgb_features': [feat[0] for feat in xgb_top_features[:10]],\n",
        "        'top_nn_features': [feat[0] for feat in nn_top_features[:10]],\n",
        "        'ft_transformer_shap': 'not_computed'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save all models\n",
        "print(\"Saving enhanced artifacts...\")\n",
        "\n",
        "# 1. Save XGBoost models\n",
        "best_xgb_model.save_model((output_dir / 'optimized_xgboost_enhanced.json').as_posix())\n",
        "print(\" Saved optimized XGBoost model\")\n",
        "\n",
        "# 2. Save PyTorch models\n",
        "torch.save({\n",
        "    'autoencoder_state_dict': autoencoder.state_dict(),\n",
        "    'autoencoder_architecture': {\n",
        "        'input_dim': n_features,\n",
        "        'encoding_dim': encoding_dim,\n",
        "        'dropout_rate': 0.2\n",
        "    },\n",
        "    'scaler_params': {\n",
        "        'mean': scaler.mean_,\n",
        "        'scale': scaler.scale_\n",
        "    }\n",
        "}, output_dir / 'autoencoder_complete.pth')\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': final_nn_model.state_dict(),\n",
        "    'model_architecture': {\n",
        "        'input_dim': enhanced_input_dim,\n",
        "        'hidden_dims': best_hidden_dims,\n",
        "        'dropout_rate': best_nn_params['dropout_rate'],\n",
        "        'use_batch_norm': best_nn_params['use_batch_norm']\n",
        "    },\n",
        "    'best_params': best_nn_params\n",
        "}, output_dir / 'neural_network_complete.pth')\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': ft_model.state_dict(),\n",
        "    'model_architecture': {\n",
        "        'n_features': enhanced_input_dim,\n",
        "        'd_token': ft_config['d_token'],\n",
        "        'n_heads': ft_config['n_heads'],\n",
        "        'n_layers': ft_config['n_layers'],\n",
        "        'dropout': ft_config['dropout'],\n",
        "        'ffn_d_hidden': ft_config['ffn_d_hidden']\n",
        "    },\n",
        "    'training': {\n",
        "        'best_val_auc_pr': ft_best_val_auc,\n",
        "        'config': ft_config\n",
        "    }\n",
        "}, output_dir / 'ft_transformer_complete.pth')\n",
        "\n",
        "print(\" Saved autoencoder, neural network, and FT-Transformer models\")\n",
        "\n",
        "# 3. Save preprocessing components\n",
        "joblib.dump(scaler, output_dir / 'feature_scaler.pkl')\n",
        "print(\" Saved feature scaler\")\n",
        "\n",
        "# 4. Save Optuna studies\n",
        "joblib.dump(study_xgb, output_dir / 'optuna_study_xgboost.pkl')\n",
        "joblib.dump(study_nn, output_dir / 'optuna_study_neural_network.pkl')\n",
        "print(\" Saved Optuna studies\")\n",
        "\n",
        "# 5. Save comprehensive metadata\n",
        "with open(output_dir / 'enhanced_experiment_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2, default=str)\n",
        "print(\" Saved comprehensive metadata\")\n",
        "\n",
        "# 6. Save model comparison\n",
        "model_comparison.to_csv(output_dir / 'model_performance_comparison.csv', index=False)\n",
        "print(\" Saved model performance comparison\")\n",
        "\n",
        "# Create summary report\n",
        "summary_report = f\"\"\"\n",
        "ENHANCED AMTTP RAPIDS GPU TRAINING - EXPERIMENT SUMMARY\n",
        "=====================================================\n",
        "Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "DATASET INFORMATION:\n",
        "- Total samples: {len(X_train) + len(X_val) + len(X_test):,}\n",
        "- Original features: {n_features}\n",
        "- Enhanced features: {X_train_enhanced.shape[1]} (+ {X_train_enhanced.shape[1] - n_features} autoencoder features)\n",
        "- Train/Val/Test split: {len(X_train):,} / {len(X_val):,} / {len(X_test):,}\n",
        "\n",
        "MODEL PERFORMANCE SUMMARY (Test Set):\n",
        "=====================================\n",
        "\n",
        "1. Original XGBoost:\n",
        "   - AUC-ROC: {metrics['auc_roc']:.4f}\n",
        "   - AUC-PR:  {metrics['auc_pr']:.4f}\n",
        "   - F1:      {metrics['f1']:.4f}\n",
        "\n",
        "2. Optimized XGBoost + Enhanced Features:\n",
        "   - AUC-ROC: {xgb_opt_metrics['auc_roc']:.4f} ({((xgb_opt_metrics['auc_roc'] - metrics['auc_roc'])/max(metrics['auc_roc'], 1e-8)*100):+.2f}%)\n",
        "   - AUC-PR:  {xgb_opt_metrics['auc_pr']:.4f} ({((xgb_opt_metrics['auc_pr'] - metrics['auc_pr'])/max(metrics['auc_pr'], 1e-8)*100):+.2f}%)\n",
        "   - F1:      {xgb_opt_metrics['f1']:.4f} ({((xgb_opt_metrics['f1'] - metrics['f1'])/max(metrics['f1'], 1e-8)*100):+.2f}%)\n",
        "\n",
        "3. Neural Network + Enhanced Features:\n",
        "   - AUC-ROC: {nn_metrics['auc_roc']:.4f} ({((nn_metrics['auc_roc'] - metrics['auc_roc'])/max(metrics['auc_roc'], 1e-8)*100):+.2f}%)\n",
        "   - AUC-PR:  {nn_metrics['auc_pr']:.4f} ({((nn_metrics['auc_pr'] - metrics['auc_pr'])/max(metrics['auc_pr'], 1e-8)*100):+.2f}%)\n",
        "   - F1:      {nn_metrics['f1']:.4f} ({((nn_metrics['f1'] - metrics['f1'])/max(metrics['f1'], 1e-8)*100):+.2f}%)\n",
        "\n",
        "4. FT-Transformer + Enhanced Features:\n",
        "   - AUC-ROC: {ft_metrics['auc_roc']:.4f} ({((ft_metrics['auc_roc'] - metrics['auc_roc'])/max(metrics['auc_roc'], 1e-8)*100):+.2f}%)\n",
        "   - AUC-PR:  {ft_metrics['auc_pr']:.4f} ({((ft_metrics['auc_pr'] - metrics['auc_pr'])/max(metrics['auc_pr'], 1e-8)*100):+.2f}%)\n",
        "   - F1:      {ft_metrics['f1']:.4f} ({((ft_metrics['f1'] - metrics['f1'])/max(metrics['f1'], 1e-8)*100):+.2f}%)\n",
        "\n",
        "BEST PERFORMING MODEL: {model_comparison.loc[model_comparison['AUC-PR'].idxmax(), 'Model']}\n",
        "\n",
        "HYPERPARAMETER OPTIMIZATION:\n",
        "============================\n",
        "- XGBoost trials: {len(study_xgb.trials)}\n",
        "- Neural Network trials: {len(study_nn.trials)}\n",
        "- FT-Transformer tuning: manual configuration (Optuna integration pending)\n",
        "- Total optimization time: ~2.5 hours\n",
        "\n",
        "KEY INSIGHTS:\n",
        "=============\n",
        "- Autoencoder features provided {X_train_enhanced.shape[1] - n_features} additional compressed representations\n",
        "- Hyperparameter tuning improved performance across all metrics\n",
        "- SHAP analysis revealed most important features for model interpretability\n",
        "- FT-Transformer delivers an attention-based alternative for tabular data leveraging the same GPU pipeline\n",
        "\n",
        "FILES SAVED:\n",
        "============\n",
        "- optimized_xgboost_enhanced.json\n",
        "- autoencoder_complete.pth\n",
        "- neural_network_complete.pth\n",
        "- ft_transformer_complete.pth\n",
        "- best_ft_transformer.pth\n",
        "- feature_scaler.pkl\n",
        "- optuna_study_*.pkl\n",
        "- enhanced_experiment_metadata.json\n",
        "- model_performance_comparison.csv\n",
        "- shap_analysis.png\n",
        "- shap_values_*.npy\n",
        "\n",
        "All artifacts saved to: {output_dir}\n",
        "\"\"\"\n",
        "\n",
        "with open(output_dir / 'experiment_summary_report.txt', 'w') as f:\n",
        "    f.write(summary_report)\n",
        "\n",
        "print(summary_report)\n",
        "print(f\"\\n Enhanced experiment completed successfully!\")\n",
        "print(f\" All artifacts saved to: {output_dir}\")\n",
        "print(f\" {len(study_xgb.trials) + len(study_nn.trials)} total hyperparameter optimization trials completed\")\n",
        "print(f\" Enhanced features: {X_train_enhanced.shape[1]} (original: {n_features} + autoencoder: {X_train_enhanced.shape[1] - n_features})\")\n",
        "\n",
        "# Optional: Create downloadable zip file (uncomment for Colab)\n",
        "# !zip -r /content/enhanced_models_results.zip {output_dir.as_posix()}\n",
        "# from google.colab import files\n",
        "# files.download('/content/enhanced_models_results.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86ad4c5f",
      "metadata": {
        "id": "86ad4c5f"
      },
      "source": [
        "## 10) Save artifacts and optionally download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bc43eb0",
      "metadata": {
        "id": "5bc43eb0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "model_path = output_dir / 'xgb_gpu_rapids.json'\n",
        "meta_path = output_dir / 'xgb_gpu_rapids_meta.json'\n",
        "bst.save_model(model_path.as_posix())\n",
        "with open(meta_path, 'w') as f:\n",
        "    json.dump({\n",
        "        'best_model': 'xgboost_gpu',\n",
        "        'dataset': dataset_path,\n",
        "        'rows_train': int(dtrain.num_row()),\n",
        "        'rows_val': int(dval.num_row()),\n",
        "        'rows_test': int(dtest.num_row()),\n",
        "        'scale_pos_weight': float(scale_pos_weight),\n",
        "        'params': {\n",
        "            'max_bin': max_bin, 'rounds': rounds, 'early': early,\n",
        "            'max_depth': max_depth, 'learning_rate': learning_rate,\n",
        "            'subsample': subsample, 'colsample_bytree': colsample_bytree\n",
        "        },\n",
        "        'metrics': metrics,\n",
        "        'categorical_features': [c for c in feature_types if c == 'categorical']\n",
        "    }, f, indent=2)\n",
        "print('Saved:', model_path)\n",
        "print('Saved:', meta_path)\n",
        "\n",
        "# Optional: zip & download\n",
        "# from google.colab import files\n",
        "# !zip -r -q /content/models_results_rapids.zip /content/models/rapids\n",
        "# files.download('/content/models_results_rapids.zip')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}